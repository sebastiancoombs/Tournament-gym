{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\IDrive-Sync\\MetaLocalLLC\\RL-Bots\\Tournament-gym\\tournamentgym.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tournamentgym import TournamentEnv\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from tqdm.autonotebook import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO,A2C,SAC,DQN,DDPG\n",
    "\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.data.rollout import TrajectoryAccumulator\n",
    "from imitation.data.huggingface_utils import trajectories_to_dataset\n",
    "# from imitation.data import serialize\n",
    "from imitation.data.rollout import rollout_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MW='M'\n",
    "\n",
    "team_data=pd.read_csv(f'Process_data/{MW}_pm_w_names.csv')\n",
    "env=TournamentEnv(team_stats=team_data,\n",
    "                     season=2024,\n",
    "                     verbose=False,\n",
    "                     loading_bar=False,\n",
    "                     shuffle=False,\n",
    "                     reward_on_round_end=False,\n",
    "                     discrete=False,\n",
    "                    #  exclude_seasons=[2023,2024]\n",
    "                    )\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env=TournamentEnv(season=2024,\n",
    "                  exclude_seasons=[c for c in range(2003,2023)],\n",
    "                  team_stats=team_data,\n",
    "                  shuffle=False,\n",
    "                  discrete=True,\n",
    "                  reward_on_round_end=False,\n",
    "                  loading_bar=False\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: using box for unknown shape rounded\n"
     ]
    }
   ],
   "source": [
    "test_env.reset(season=2024)\n",
    "done=False\n",
    "while not done:\n",
    "    act=test_env.action_space.sample()\n",
    "\n",
    "    obs,rew,done,trunc,info=test_env.step(act)\n",
    "test_env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cca67a8b4144daa8967b0e9b6e699d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_dems=1000\n",
    "traj_acum=TrajectoryAccumulator()\n",
    "demonstrations=[]\n",
    "for i in tqdm(range(num_dems)):\n",
    "    obs,info=env.reset()\n",
    "    terminated=False\n",
    "    step={'obs':obs}\n",
    "    traj_acum.add_step(key=i,step_dict=step)\n",
    "    j=0\n",
    "    while not terminated:\n",
    "        act=env.cheat_action() \n",
    "        j+=1\n",
    "        if i%10!=0:\n",
    "            if (j%10)==0:\n",
    "                act=random.choice([[1,0],[0,1]])\n",
    "        next_obs, reward, terminated,truncated, info=env.step(action=act,)\n",
    "        \n",
    "        step={'obs':np.array(obs),\n",
    "            'acts':np.array(act),\n",
    "            'rews':np.array(reward),\n",
    "            'infos':info}\n",
    "        # if i==0:\n",
    "        #     print(reward)\n",
    "        traj_acum.add_step(key=i,\n",
    "                        step_dict=step,\n",
    "                        )\n",
    "        # print(reward,train_env.trade_week_start,train_env.current_time)\n",
    "        obs=next_obs\n",
    "    \n",
    "    \n",
    "    traj=traj_acum.finish_trajectory(key=i,terminal=terminated)\n",
    "    demonstrations.append(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_dataset=trajectories_to_dataset(demonstrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284.414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " {'n_traj': 1000,\n",
       "  'return_min': 143.5,\n",
       "  'return_mean': 284.414,\n",
       "  'return_std': 38.65047999701944,\n",
       "  'return_max': 320.0,\n",
       "  'len_min': 63,\n",
       "  'len_mean': 65.652,\n",
       "  'len_std': 1.5642557335678846,\n",
       "  'len_max': 67})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = rollout_stats(demonstrations)\n",
    "print(stats[\"return_mean\"])\n",
    "torch.cuda.is_available(),stats,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 20 #subsets of the episode we use for training\n",
    "    state_dim: int = 17  # size of state space\n",
    "\n",
    "    p_sample: np.array = None  \n",
    "    # n_traj: int = 0 \n",
    "\n",
    "    def __init__(self, demonstrations) -> None:\n",
    "        traj_stats=rollout_stats(demonstrations)\n",
    "        self.n_traj=traj_stats['n_traj'] # to store the number of trajectories in the dataset\n",
    "        self.max_ep_len=traj_stats['len_max'] # max episode length in the dataset\n",
    "        self.scale = traj_stats['return_max']  # normalization of rewards/returns\n",
    "        dataset=trajectories_to_dataset(demonstrations) ## turn list of trajectories into a dataset\n",
    "        self.dataset = dataset\n",
    "        self.act_dim = len(dataset[0]['acts'][0])\n",
    "        # calculate dataset stats for normalization of states\n",
    "        states = []\n",
    "        traj_lens = []\n",
    "        for obs in dataset[\"obs\"]:\n",
    "            obs=np.array(obs)\n",
    "            states.append(obs)\n",
    "            traj_lens.append(len(obs))\n",
    "        \n",
    "        states = np.vstack(states)\n",
    "        self.state_dim=states.shape[1]\n",
    "        self.state_mean = np.mean(states, axis=0) # to store state means\n",
    "\n",
    "        self.state_std=np.std(states, axis=0) + 1e-6 # to store state stds\n",
    "        traj_lens = np.array(traj_lens)\n",
    "        self.p_sample = traj_lens / sum(traj_lens) # a distribution to take account trajectory lengths\n",
    "\n",
    "    def _discount_cumsum(self, x, gamma):\n",
    "\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_size = len(features)\n",
    "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "        batch_inds = np.random.choice(\n",
    "            np.arange(self.n_traj),\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
    "        )\n",
    "        # a batch of dataset features\n",
    "        s, a, r, rtg, timesteps, mask = [], [], [], [], [], []\n",
    "        \n",
    "        for ind in batch_inds:\n",
    "            # for feature in features:\n",
    "            feature = self.dataset[int(ind)]\n",
    "            obs=np.array(feature[\"obs\"])\n",
    "            rews=np.array(feature[\"rews\"]).reshape(-1,1)\n",
    "            acts=np.array(feature[\"acts\"])\n",
    "\n",
    "            si = random.randint(1, len(rews) -1)\n",
    "            \n",
    "\n",
    "            # get sequences from dataset\n",
    "            s.append(np.array(obs[si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
    "            a.append(np.array(acts[si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "            r.append(np.array(rews[si : si + self.max_len]).reshape(1, -1, 1))\n",
    "\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "\n",
    "            discount_rew=self._discount_cumsum(np.array(rews[si:]), gamma=1.0)\n",
    "\n",
    "            slen=s[-1].shape[1]\n",
    "\n",
    "            rew_tg=discount_rew[: slen].reshape(1, -1, 1)\n",
    "            rtg.append(rew_tg)\n",
    "            rtg_len=rtg[-1].shape[1]\n",
    "            rtg_pad=np.zeros((1,self.max_len-rtg_len, 1))\n",
    "            rtg[-1] = np.concatenate([rtg[-1],rtg_pad], axis=1)\n",
    "            rtg[-1]=rtg[-1]/ self.scale\n",
    "\n",
    "            # padding and state + reward normalization\n",
    "            s_pad=np.zeros((1, self.max_len - slen, self.state_dim))\n",
    "            s[-1] = np.concatenate([s_pad, s[-1]], axis=1)\n",
    "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
    "\n",
    "            alen=a[-1].shape[1]\n",
    "            act_pad=np.ones((1, self.max_len - alen, self.act_dim))\n",
    "            a[-1] = np.concatenate([ act_pad* -10.0, a[-1]], axis=1,)\n",
    "\n",
    "            rlen=r[-1].shape[1]\n",
    "            rew_pad=np.zeros((1, self.max_len - rlen, 1))\n",
    "            r[-1] = np.concatenate([rew_pad, r[-1]], axis=1)\n",
    "\n",
    "            tlen=timesteps[-1].shape[1]\n",
    "            time_pad=np.zeros((1, self.max_len - tlen))\n",
    "            timesteps[-1] = np.concatenate([time_pad, timesteps[-1]], axis=1)\n",
    "\n",
    "            masked=np.zeros((1, self.max_len - alen))\n",
    "            unmasked=np.ones((1, alen))\n",
    "            mask.append(np.concatenate([masked,unmasked], axis=1))\n",
    "\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "        return {\n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"returns_to_go\": rtg,\n",
    "            \"timesteps\": timesteps,\n",
    "            \"attention_mask\": mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableDT(DecisionTransformerModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # self.soft=torch.nn.Softmax()\n",
    "        self.loss_func= torch.nn.functional.binary_cross_entropy_with_logits\n",
    "        \n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        output = super().forward(**kwargs)\n",
    "        # add the DT loss\n",
    "        action_preds = output[1]\n",
    "        action_targets = kwargs[\"actions\"]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        act_dim = action_preds.shape[2]\n",
    "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        # action_preds=self.soft(action_preds)        \n",
    "        loss = self.loss_func(action_preds ,action_targets,) \n",
    "\n",
    "        # loss=torch.mean(losses)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def original_forward(self, **kwargs):\n",
    "        return super().forward(**kwargs)\n",
    "    \n",
    "    def predict(self, states, actions, rewards, returns_to_go, timesteps):\n",
    "        # This implementation does not condition on past rewards\n",
    "\n",
    "        states = states.reshape(1, -1, self.config.state_dim)\n",
    "        actions = actions.reshape(1, -1, self.config.act_dim)\n",
    "        returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
    "        timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "        states = states[:, -self.config.max_length :]\n",
    "        actions = actions[:, -self.config.max_length :]\n",
    "        returns_to_go = returns_to_go[:, -self.config.max_length :]\n",
    "        timesteps = timesteps[:, -self.config.max_length :]\n",
    "        padding = self.config.max_length - states.shape[1]\n",
    "        # pad all tokens to sequence length\n",
    "        attention_mask = torch.cat([torch.zeros(padding), torch.ones(states.shape[1])])\n",
    "        attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
    "        states = torch.cat([torch.zeros((1, padding, self.config.state_dim)), states], dim=1).float()\n",
    "        returns_to_go = torch.cat([torch.zeros((1, padding, 1)), returns_to_go], dim=1).float()\n",
    "        timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long), timesteps], dim=1)\n",
    "\n",
    "        state_preds, action_preds, return_preds = self.original_forward(\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            returns_to_go=returns_to_go,\n",
    "            timesteps=timesteps,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False,\n",
    "        )\n",
    "        action_preds=self.soft(action_preds) \n",
    "        action=action_preds[0, -1]\n",
    "\n",
    "        return action\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "collator = DecisionTransformerGymDataCollator(demonstrations)\n",
    "\n",
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = TrainableDT(config)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17e0b42c70841f0b013ebefabd137ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6975, 'learning_rate': 1e-05, 'epoch': 1.0}\n",
      "{'loss': 0.6954, 'learning_rate': 2e-05, 'epoch': 2.0}\n",
      "{'loss': 0.6926, 'learning_rate': 3e-05, 'epoch': 3.0}\n",
      "{'loss': 0.6908, 'learning_rate': 4e-05, 'epoch': 4.0}\n",
      "{'loss': 0.6861, 'learning_rate': 5e-05, 'epoch': 5.0}\n",
      "{'loss': 0.6816, 'learning_rate': 6e-05, 'epoch': 6.0}\n",
      "{'loss': 0.6768, 'learning_rate': 7.000000000000001e-05, 'epoch': 7.0}\n",
      "{'loss': 0.6715, 'learning_rate': 8e-05, 'epoch': 8.0}\n",
      "{'loss': 0.6642, 'learning_rate': 8.999999999999999e-05, 'epoch': 9.0}\n",
      "{'loss': 0.6522, 'learning_rate': 0.0001, 'epoch': 10.0}\n",
      "{'loss': 0.6359, 'learning_rate': 0.00011, 'epoch': 11.0}\n",
      "{'loss': 0.6138, 'learning_rate': 0.00012, 'epoch': 12.0}\n",
      "{'loss': 0.5958, 'learning_rate': 0.00013000000000000002, 'epoch': 13.0}\n",
      "{'loss': 0.575, 'learning_rate': 0.00014000000000000001, 'epoch': 14.0}\n",
      "{'loss': 0.5506, 'learning_rate': 0.00015, 'epoch': 15.0}\n",
      "{'loss': 0.5323, 'learning_rate': 0.00016, 'epoch': 16.0}\n",
      "{'loss': 0.5125, 'learning_rate': 0.00017, 'epoch': 17.0}\n",
      "{'loss': 0.4959, 'learning_rate': 0.00017999999999999998, 'epoch': 18.0}\n",
      "{'loss': 0.4916, 'learning_rate': 0.00019, 'epoch': 19.0}\n",
      "{'loss': 0.4909, 'learning_rate': 0.0002, 'epoch': 20.0}\n",
      "{'loss': 0.4751, 'learning_rate': 0.00021, 'epoch': 21.0}\n",
      "{'loss': 0.4708, 'learning_rate': 0.00022, 'epoch': 22.0}\n",
      "{'loss': 0.4608, 'learning_rate': 0.00023, 'epoch': 23.0}\n",
      "{'loss': 0.4609, 'learning_rate': 0.00024, 'epoch': 24.0}\n",
      "{'loss': 0.4604, 'learning_rate': 0.00025, 'epoch': 25.0}\n",
      "{'loss': 0.4534, 'learning_rate': 0.00026000000000000003, 'epoch': 26.0}\n",
      "{'loss': 0.4573, 'learning_rate': 0.00027, 'epoch': 27.0}\n",
      "{'loss': 0.4536, 'learning_rate': 0.00028000000000000003, 'epoch': 28.0}\n",
      "{'loss': 0.4458, 'learning_rate': 0.00029, 'epoch': 29.0}\n",
      "{'loss': 0.4467, 'learning_rate': 0.0003, 'epoch': 30.0}\n",
      "{'loss': 0.4475, 'learning_rate': 0.00031, 'epoch': 31.0}\n",
      "{'loss': 0.4478, 'learning_rate': 0.00032, 'epoch': 32.0}\n",
      "{'loss': 0.44, 'learning_rate': 0.00033, 'epoch': 33.0}\n",
      "{'loss': 0.4392, 'learning_rate': 0.00034, 'epoch': 34.0}\n",
      "{'loss': 0.4402, 'learning_rate': 0.00035, 'epoch': 35.0}\n",
      "{'loss': 0.4398, 'learning_rate': 0.00035999999999999997, 'epoch': 36.0}\n",
      "{'loss': 0.4371, 'learning_rate': 0.00037, 'epoch': 37.0}\n",
      "{'loss': 0.4382, 'learning_rate': 0.00038, 'epoch': 38.0}\n",
      "{'loss': 0.4317, 'learning_rate': 0.00039000000000000005, 'epoch': 39.0}\n",
      "{'loss': 0.4366, 'learning_rate': 0.0004, 'epoch': 40.0}\n",
      "{'loss': 0.4263, 'learning_rate': 0.00041, 'epoch': 41.0}\n",
      "{'loss': 0.429, 'learning_rate': 0.00042, 'epoch': 42.0}\n",
      "{'loss': 0.4315, 'learning_rate': 0.00043, 'epoch': 43.0}\n",
      "{'loss': 0.4257, 'learning_rate': 0.00044, 'epoch': 44.0}\n",
      "{'loss': 0.4178, 'learning_rate': 0.00045000000000000004, 'epoch': 45.0}\n",
      "{'loss': 0.4231, 'learning_rate': 0.00046, 'epoch': 46.0}\n",
      "{'loss': 0.4202, 'learning_rate': 0.00047, 'epoch': 47.0}\n",
      "{'loss': 0.4269, 'learning_rate': 0.00048, 'epoch': 48.0}\n",
      "{'loss': 0.4212, 'learning_rate': 0.00049, 'epoch': 49.0}\n",
      "{'loss': 0.4205, 'learning_rate': 0.0005, 'epoch': 50.0}\n",
      "{'loss': 0.4225, 'learning_rate': 0.00051, 'epoch': 51.0}\n",
      "{'loss': 0.4176, 'learning_rate': 0.0005200000000000001, 'epoch': 52.0}\n",
      "{'loss': 0.4156, 'learning_rate': 0.0005300000000000001, 'epoch': 53.0}\n",
      "{'loss': 0.4188, 'learning_rate': 0.00054, 'epoch': 54.0}\n",
      "{'loss': 0.4164, 'learning_rate': 0.00055, 'epoch': 55.0}\n",
      "{'loss': 0.4159, 'learning_rate': 0.0005600000000000001, 'epoch': 56.0}\n",
      "{'loss': 0.4156, 'learning_rate': 0.00057, 'epoch': 57.0}\n",
      "{'loss': 0.4114, 'learning_rate': 0.00058, 'epoch': 58.0}\n",
      "{'loss': 0.4137, 'learning_rate': 0.00059, 'epoch': 59.0}\n",
      "{'loss': 0.4186, 'learning_rate': 0.0006, 'epoch': 60.0}\n",
      "{'loss': 0.4146, 'learning_rate': 0.00061, 'epoch': 61.0}\n",
      "{'loss': 0.4039, 'learning_rate': 0.00062, 'epoch': 62.0}\n",
      "{'loss': 0.41, 'learning_rate': 0.00063, 'epoch': 63.0}\n",
      "{'loss': 0.4091, 'learning_rate': 0.00064, 'epoch': 64.0}\n",
      "{'loss': 0.4083, 'learning_rate': 0.0006500000000000001, 'epoch': 65.0}\n",
      "{'loss': 0.4126, 'learning_rate': 0.00066, 'epoch': 66.0}\n",
      "{'loss': 0.4057, 'learning_rate': 0.00067, 'epoch': 67.0}\n",
      "{'loss': 0.4017, 'learning_rate': 0.00068, 'epoch': 68.0}\n",
      "{'loss': 0.409, 'learning_rate': 0.00069, 'epoch': 69.0}\n",
      "{'loss': 0.4049, 'learning_rate': 0.0007, 'epoch': 70.0}\n",
      "{'loss': 0.4077, 'learning_rate': 0.00071, 'epoch': 71.0}\n",
      "{'loss': 0.4094, 'learning_rate': 0.0007199999999999999, 'epoch': 72.0}\n",
      "{'loss': 0.4061, 'learning_rate': 0.00073, 'epoch': 73.0}\n",
      "{'loss': 0.4061, 'learning_rate': 0.00074, 'epoch': 74.0}\n",
      "{'loss': 0.4038, 'learning_rate': 0.00075, 'epoch': 75.0}\n",
      "{'loss': 0.4017, 'learning_rate': 0.00076, 'epoch': 76.0}\n",
      "{'loss': 0.4054, 'learning_rate': 0.0007700000000000001, 'epoch': 77.0}\n",
      "{'loss': 0.4075, 'learning_rate': 0.0007800000000000001, 'epoch': 78.0}\n",
      "{'loss': 0.4018, 'learning_rate': 0.00079, 'epoch': 79.0}\n",
      "{'loss': 0.398, 'learning_rate': 0.0008, 'epoch': 80.0}\n",
      "{'loss': 0.4075, 'learning_rate': 0.0008100000000000001, 'epoch': 81.0}\n",
      "{'loss': 0.4023, 'learning_rate': 0.00082, 'epoch': 82.0}\n",
      "{'loss': 0.4025, 'learning_rate': 0.00083, 'epoch': 83.0}\n",
      "{'loss': 0.3977, 'learning_rate': 0.00084, 'epoch': 84.0}\n",
      "{'loss': 0.4041, 'learning_rate': 0.00085, 'epoch': 85.0}\n",
      "{'loss': 0.4036, 'learning_rate': 0.00086, 'epoch': 86.0}\n",
      "{'loss': 0.4011, 'learning_rate': 0.00087, 'epoch': 87.0}\n",
      "{'loss': 0.4057, 'learning_rate': 0.00088, 'epoch': 88.0}\n",
      "{'loss': 0.4024, 'learning_rate': 0.0008900000000000001, 'epoch': 89.0}\n",
      "{'loss': 0.4021, 'learning_rate': 0.0009000000000000001, 'epoch': 90.0}\n",
      "{'loss': 0.4042, 'learning_rate': 0.00091, 'epoch': 91.0}\n",
      "{'loss': 0.401, 'learning_rate': 0.00092, 'epoch': 92.0}\n",
      "{'loss': 0.4006, 'learning_rate': 0.00093, 'epoch': 93.0}\n",
      "{'loss': 0.3994, 'learning_rate': 0.00094, 'epoch': 94.0}\n",
      "{'loss': 0.3994, 'learning_rate': 0.00095, 'epoch': 95.0}\n",
      "{'loss': 0.3991, 'learning_rate': 0.00096, 'epoch': 96.0}\n",
      "{'loss': 0.4003, 'learning_rate': 0.0009699999999999999, 'epoch': 97.0}\n",
      "{'loss': 0.4008, 'learning_rate': 0.00098, 'epoch': 98.0}\n",
      "{'loss': 0.3971, 'learning_rate': 0.00099, 'epoch': 99.0}\n",
      "{'loss': 0.3977, 'learning_rate': 0.001, 'epoch': 100.0}\n",
      "{'loss': 0.3995, 'learning_rate': 0.000998888888888889, 'epoch': 101.0}\n",
      "{'loss': 0.3952, 'learning_rate': 0.0009977777777777778, 'epoch': 102.0}\n",
      "{'loss': 0.4008, 'learning_rate': 0.0009966666666666668, 'epoch': 103.0}\n",
      "{'loss': 0.3964, 'learning_rate': 0.0009955555555555555, 'epoch': 104.0}\n",
      "{'loss': 0.4003, 'learning_rate': 0.0009944444444444445, 'epoch': 105.0}\n",
      "{'loss': 0.395, 'learning_rate': 0.0009933333333333333, 'epoch': 106.0}\n",
      "{'loss': 0.3991, 'learning_rate': 0.0009922222222222222, 'epoch': 107.0}\n",
      "{'loss': 0.397, 'learning_rate': 0.0009911111111111112, 'epoch': 108.0}\n",
      "{'loss': 0.3927, 'learning_rate': 0.00099, 'epoch': 109.0}\n",
      "{'loss': 0.3954, 'learning_rate': 0.000988888888888889, 'epoch': 110.0}\n",
      "{'loss': 0.3987, 'learning_rate': 0.0009877777777777777, 'epoch': 111.0}\n",
      "{'loss': 0.3896, 'learning_rate': 0.0009866666666666667, 'epoch': 112.0}\n",
      "{'loss': 0.3908, 'learning_rate': 0.0009855555555555555, 'epoch': 113.0}\n",
      "{'loss': 0.3925, 'learning_rate': 0.0009844444444444445, 'epoch': 114.0}\n",
      "{'loss': 0.3918, 'learning_rate': 0.0009833333333333332, 'epoch': 115.0}\n",
      "{'loss': 0.391, 'learning_rate': 0.0009822222222222222, 'epoch': 116.0}\n",
      "{'loss': 0.3918, 'learning_rate': 0.0009811111111111112, 'epoch': 117.0}\n",
      "{'loss': 0.3961, 'learning_rate': 0.00098, 'epoch': 118.0}\n",
      "{'loss': 0.3959, 'learning_rate': 0.000978888888888889, 'epoch': 119.0}\n",
      "{'loss': 0.3917, 'learning_rate': 0.0009777777777777777, 'epoch': 120.0}\n",
      "{'loss': 0.3915, 'learning_rate': 0.0009766666666666667, 'epoch': 121.0}\n",
      "{'loss': 0.3941, 'learning_rate': 0.0009755555555555556, 'epoch': 122.0}\n",
      "{'loss': 0.3935, 'learning_rate': 0.0009744444444444444, 'epoch': 123.0}\n",
      "{'loss': 0.3882, 'learning_rate': 0.0009733333333333334, 'epoch': 124.0}\n",
      "{'loss': 0.3943, 'learning_rate': 0.0009722222222222222, 'epoch': 125.0}\n",
      "{'loss': 0.3911, 'learning_rate': 0.0009711111111111112, 'epoch': 126.0}\n",
      "{'loss': 0.3921, 'learning_rate': 0.0009699999999999999, 'epoch': 127.0}\n",
      "{'loss': 0.3889, 'learning_rate': 0.0009688888888888889, 'epoch': 128.0}\n",
      "{'loss': 0.3846, 'learning_rate': 0.0009677777777777778, 'epoch': 129.0}\n",
      "{'loss': 0.3892, 'learning_rate': 0.0009666666666666667, 'epoch': 130.0}\n",
      "{'loss': 0.3861, 'learning_rate': 0.0009655555555555555, 'epoch': 131.0}\n",
      "{'loss': 0.3851, 'learning_rate': 0.0009644444444444444, 'epoch': 132.0}\n",
      "{'loss': 0.3842, 'learning_rate': 0.0009633333333333334, 'epoch': 133.0}\n",
      "{'loss': 0.3851, 'learning_rate': 0.0009622222222222222, 'epoch': 134.0}\n",
      "{'loss': 0.3858, 'learning_rate': 0.0009611111111111112, 'epoch': 135.0}\n",
      "{'loss': 0.3847, 'learning_rate': 0.00096, 'epoch': 136.0}\n",
      "{'loss': 0.3845, 'learning_rate': 0.0009588888888888889, 'epoch': 137.0}\n",
      "{'loss': 0.3839, 'learning_rate': 0.0009577777777777778, 'epoch': 138.0}\n",
      "{'loss': 0.3869, 'learning_rate': 0.0009566666666666666, 'epoch': 139.0}\n",
      "{'loss': 0.3798, 'learning_rate': 0.0009555555555555556, 'epoch': 140.0}\n",
      "{'loss': 0.3865, 'learning_rate': 0.0009544444444444445, 'epoch': 141.0}\n",
      "{'loss': 0.3847, 'learning_rate': 0.0009533333333333334, 'epoch': 142.0}\n",
      "{'loss': 0.3839, 'learning_rate': 0.0009522222222222223, 'epoch': 143.0}\n",
      "{'loss': 0.3835, 'learning_rate': 0.0009511111111111111, 'epoch': 144.0}\n",
      "{'loss': 0.3842, 'learning_rate': 0.00095, 'epoch': 145.0}\n",
      "{'loss': 0.3811, 'learning_rate': 0.0009488888888888889, 'epoch': 146.0}\n",
      "{'loss': 0.3775, 'learning_rate': 0.0009477777777777779, 'epoch': 147.0}\n",
      "{'loss': 0.3834, 'learning_rate': 0.0009466666666666667, 'epoch': 148.0}\n",
      "{'loss': 0.379, 'learning_rate': 0.0009455555555555556, 'epoch': 149.0}\n",
      "{'loss': 0.3833, 'learning_rate': 0.0009444444444444445, 'epoch': 150.0}\n",
      "{'loss': 0.3821, 'learning_rate': 0.0009433333333333334, 'epoch': 151.0}\n",
      "{'loss': 0.377, 'learning_rate': 0.0009422222222222222, 'epoch': 152.0}\n",
      "{'loss': 0.3768, 'learning_rate': 0.0009411111111111111, 'epoch': 153.0}\n",
      "{'loss': 0.3776, 'learning_rate': 0.00094, 'epoch': 154.0}\n",
      "{'loss': 0.3805, 'learning_rate': 0.000938888888888889, 'epoch': 155.0}\n",
      "{'loss': 0.3793, 'learning_rate': 0.0009377777777777778, 'epoch': 156.0}\n",
      "{'loss': 0.3796, 'learning_rate': 0.0009366666666666667, 'epoch': 157.0}\n",
      "{'loss': 0.3806, 'learning_rate': 0.0009355555555555556, 'epoch': 158.0}\n",
      "{'loss': 0.3821, 'learning_rate': 0.0009344444444444444, 'epoch': 159.0}\n",
      "{'loss': 0.3819, 'learning_rate': 0.0009333333333333333, 'epoch': 160.0}\n",
      "{'loss': 0.3794, 'learning_rate': 0.0009322222222222222, 'epoch': 161.0}\n",
      "{'loss': 0.3802, 'learning_rate': 0.0009311111111111112, 'epoch': 162.0}\n",
      "{'loss': 0.3805, 'learning_rate': 0.00093, 'epoch': 163.0}\n",
      "{'loss': 0.377, 'learning_rate': 0.0009288888888888889, 'epoch': 164.0}\n",
      "{'loss': 0.3773, 'learning_rate': 0.0009277777777777778, 'epoch': 165.0}\n",
      "{'loss': 0.3745, 'learning_rate': 0.0009266666666666667, 'epoch': 166.0}\n",
      "{'loss': 0.377, 'learning_rate': 0.0009255555555555555, 'epoch': 167.0}\n",
      "{'loss': 0.3748, 'learning_rate': 0.0009244444444444444, 'epoch': 168.0}\n",
      "{'loss': 0.3741, 'learning_rate': 0.0009233333333333334, 'epoch': 169.0}\n",
      "{'loss': 0.374, 'learning_rate': 0.0009222222222222223, 'epoch': 170.0}\n",
      "{'loss': 0.3767, 'learning_rate': 0.0009211111111111112, 'epoch': 171.0}\n",
      "{'loss': 0.3782, 'learning_rate': 0.00092, 'epoch': 172.0}\n",
      "{'loss': 0.3767, 'learning_rate': 0.0009188888888888889, 'epoch': 173.0}\n",
      "{'loss': 0.3772, 'learning_rate': 0.0009177777777777778, 'epoch': 174.0}\n",
      "{'loss': 0.376, 'learning_rate': 0.0009166666666666666, 'epoch': 175.0}\n",
      "{'loss': 0.377, 'learning_rate': 0.0009155555555555556, 'epoch': 176.0}\n",
      "{'loss': 0.3744, 'learning_rate': 0.0009144444444444444, 'epoch': 177.0}\n",
      "{'loss': 0.374, 'learning_rate': 0.0009133333333333334, 'epoch': 178.0}\n",
      "{'loss': 0.3757, 'learning_rate': 0.0009122222222222223, 'epoch': 179.0}\n",
      "{'loss': 0.3727, 'learning_rate': 0.0009111111111111111, 'epoch': 180.0}\n",
      "{'loss': 0.3732, 'learning_rate': 0.00091, 'epoch': 181.0}\n",
      "{'loss': 0.3725, 'learning_rate': 0.0009088888888888889, 'epoch': 182.0}\n",
      "{'loss': 0.3708, 'learning_rate': 0.0009077777777777779, 'epoch': 183.0}\n",
      "{'loss': 0.3716, 'learning_rate': 0.0009066666666666666, 'epoch': 184.0}\n",
      "{'loss': 0.3742, 'learning_rate': 0.0009055555555555556, 'epoch': 185.0}\n",
      "{'loss': 0.3727, 'learning_rate': 0.0009044444444444445, 'epoch': 186.0}\n",
      "{'loss': 0.372, 'learning_rate': 0.0009033333333333334, 'epoch': 187.0}\n",
      "{'loss': 0.373, 'learning_rate': 0.0009022222222222222, 'epoch': 188.0}\n",
      "{'loss': 0.372, 'learning_rate': 0.0009011111111111111, 'epoch': 189.0}\n",
      "{'loss': 0.3743, 'learning_rate': 0.0009000000000000001, 'epoch': 190.0}\n",
      "{'loss': 0.3729, 'learning_rate': 0.0008988888888888888, 'epoch': 191.0}\n",
      "{'loss': 0.3706, 'learning_rate': 0.0008977777777777778, 'epoch': 192.0}\n",
      "{'loss': 0.3715, 'learning_rate': 0.0008966666666666666, 'epoch': 193.0}\n",
      "{'loss': 0.3706, 'learning_rate': 0.0008955555555555556, 'epoch': 194.0}\n",
      "{'loss': 0.3725, 'learning_rate': 0.0008944444444444445, 'epoch': 195.0}\n",
      "{'loss': 0.371, 'learning_rate': 0.0008933333333333333, 'epoch': 196.0}\n",
      "{'loss': 0.37, 'learning_rate': 0.0008922222222222223, 'epoch': 197.0}\n",
      "{'loss': 0.3676, 'learning_rate': 0.0008911111111111111, 'epoch': 198.0}\n",
      "{'loss': 0.3742, 'learning_rate': 0.0008900000000000001, 'epoch': 199.0}\n",
      "{'loss': 0.3681, 'learning_rate': 0.0008888888888888888, 'epoch': 200.0}\n",
      "{'loss': 0.3703, 'learning_rate': 0.0008877777777777778, 'epoch': 201.0}\n",
      "{'loss': 0.3696, 'learning_rate': 0.0008866666666666667, 'epoch': 202.0}\n",
      "{'loss': 0.3734, 'learning_rate': 0.0008855555555555556, 'epoch': 203.0}\n",
      "{'loss': 0.371, 'learning_rate': 0.0008844444444444445, 'epoch': 204.0}\n",
      "{'loss': 0.3733, 'learning_rate': 0.0008833333333333333, 'epoch': 205.0}\n",
      "{'loss': 0.3689, 'learning_rate': 0.0008822222222222223, 'epoch': 206.0}\n",
      "{'loss': 0.3721, 'learning_rate': 0.000881111111111111, 'epoch': 207.0}\n",
      "{'loss': 0.372, 'learning_rate': 0.00088, 'epoch': 208.0}\n",
      "{'loss': 0.3727, 'learning_rate': 0.000878888888888889, 'epoch': 209.0}\n",
      "{'loss': 0.3672, 'learning_rate': 0.0008777777777777778, 'epoch': 210.0}\n",
      "{'loss': 0.3718, 'learning_rate': 0.0008766666666666668, 'epoch': 211.0}\n",
      "{'loss': 0.3701, 'learning_rate': 0.0008755555555555555, 'epoch': 212.0}\n",
      "{'loss': 0.3724, 'learning_rate': 0.0008744444444444445, 'epoch': 213.0}\n",
      "{'loss': 0.3695, 'learning_rate': 0.0008733333333333333, 'epoch': 214.0}\n",
      "{'loss': 0.37, 'learning_rate': 0.0008722222222222223, 'epoch': 215.0}\n",
      "{'loss': 0.3687, 'learning_rate': 0.000871111111111111, 'epoch': 216.0}\n",
      "{'loss': 0.3697, 'learning_rate': 0.00087, 'epoch': 217.0}\n",
      "{'loss': 0.3682, 'learning_rate': 0.000868888888888889, 'epoch': 218.0}\n",
      "{'loss': 0.3681, 'learning_rate': 0.0008677777777777778, 'epoch': 219.0}\n",
      "{'loss': 0.3709, 'learning_rate': 0.0008666666666666667, 'epoch': 220.0}\n",
      "{'loss': 0.3696, 'learning_rate': 0.0008655555555555555, 'epoch': 221.0}\n",
      "{'loss': 0.3661, 'learning_rate': 0.0008644444444444445, 'epoch': 222.0}\n",
      "{'loss': 0.3679, 'learning_rate': 0.0008633333333333334, 'epoch': 223.0}\n",
      "{'loss': 0.368, 'learning_rate': 0.0008622222222222222, 'epoch': 224.0}\n",
      "{'loss': 0.3693, 'learning_rate': 0.0008611111111111112, 'epoch': 225.0}\n",
      "{'loss': 0.3731, 'learning_rate': 0.00086, 'epoch': 226.0}\n",
      "{'loss': 0.3679, 'learning_rate': 0.000858888888888889, 'epoch': 227.0}\n",
      "{'loss': 0.3678, 'learning_rate': 0.0008577777777777777, 'epoch': 228.0}\n",
      "{'loss': 0.367, 'learning_rate': 0.0008566666666666667, 'epoch': 229.0}\n",
      "{'loss': 0.3666, 'learning_rate': 0.0008555555555555556, 'epoch': 230.0}\n",
      "{'loss': 0.3675, 'learning_rate': 0.0008544444444444445, 'epoch': 231.0}\n",
      "{'loss': 0.37, 'learning_rate': 0.0008533333333333334, 'epoch': 232.0}\n",
      "{'loss': 0.3643, 'learning_rate': 0.0008522222222222222, 'epoch': 233.0}\n",
      "{'loss': 0.366, 'learning_rate': 0.0008511111111111112, 'epoch': 234.0}\n",
      "{'loss': 0.3659, 'learning_rate': 0.00085, 'epoch': 235.0}\n",
      "{'loss': 0.3658, 'learning_rate': 0.0008488888888888889, 'epoch': 236.0}\n",
      "{'loss': 0.3658, 'learning_rate': 0.0008477777777777778, 'epoch': 237.0}\n",
      "{'loss': 0.3641, 'learning_rate': 0.0008466666666666667, 'epoch': 238.0}\n",
      "{'loss': 0.3649, 'learning_rate': 0.0008455555555555556, 'epoch': 239.0}\n",
      "{'loss': 0.367, 'learning_rate': 0.0008444444444444444, 'epoch': 240.0}\n",
      "{'loss': 0.367, 'learning_rate': 0.0008433333333333334, 'epoch': 241.0}\n",
      "{'loss': 0.3679, 'learning_rate': 0.0008422222222222222, 'epoch': 242.0}\n",
      "{'loss': 0.3641, 'learning_rate': 0.0008411111111111112, 'epoch': 243.0}\n",
      "{'loss': 0.3651, 'learning_rate': 0.00084, 'epoch': 244.0}\n",
      "{'loss': 0.3659, 'learning_rate': 0.0008388888888888889, 'epoch': 245.0}\n",
      "{'loss': 0.3675, 'learning_rate': 0.0008377777777777778, 'epoch': 246.0}\n",
      "{'loss': 0.368, 'learning_rate': 0.0008366666666666667, 'epoch': 247.0}\n",
      "{'loss': 0.3648, 'learning_rate': 0.0008355555555555556, 'epoch': 248.0}\n",
      "{'loss': 0.3663, 'learning_rate': 0.0008344444444444444, 'epoch': 249.0}\n",
      "{'loss': 0.3659, 'learning_rate': 0.0008333333333333334, 'epoch': 250.0}\n",
      "{'loss': 0.3654, 'learning_rate': 0.0008322222222222223, 'epoch': 251.0}\n",
      "{'loss': 0.3648, 'learning_rate': 0.0008311111111111111, 'epoch': 252.0}\n",
      "{'loss': 0.3645, 'learning_rate': 0.00083, 'epoch': 253.0}\n",
      "{'loss': 0.3685, 'learning_rate': 0.0008288888888888889, 'epoch': 254.0}\n",
      "{'loss': 0.3667, 'learning_rate': 0.0008277777777777778, 'epoch': 255.0}\n",
      "{'loss': 0.3649, 'learning_rate': 0.0008266666666666666, 'epoch': 256.0}\n",
      "{'loss': 0.3661, 'learning_rate': 0.0008255555555555556, 'epoch': 257.0}\n",
      "{'loss': 0.3658, 'learning_rate': 0.0008244444444444445, 'epoch': 258.0}\n",
      "{'loss': 0.3645, 'learning_rate': 0.0008233333333333334, 'epoch': 259.0}\n",
      "{'loss': 0.3655, 'learning_rate': 0.0008222222222222222, 'epoch': 260.0}\n",
      "{'loss': 0.3633, 'learning_rate': 0.0008211111111111111, 'epoch': 261.0}\n",
      "{'loss': 0.3648, 'learning_rate': 0.00082, 'epoch': 262.0}\n",
      "{'loss': 0.3645, 'learning_rate': 0.0008188888888888889, 'epoch': 263.0}\n",
      "{'loss': 0.364, 'learning_rate': 0.0008177777777777778, 'epoch': 264.0}\n",
      "{'loss': 0.3616, 'learning_rate': 0.0008166666666666667, 'epoch': 265.0}\n",
      "{'loss': 0.3636, 'learning_rate': 0.0008155555555555556, 'epoch': 266.0}\n",
      "{'loss': 0.3685, 'learning_rate': 0.0008144444444444445, 'epoch': 267.0}\n",
      "{'loss': 0.3656, 'learning_rate': 0.0008133333333333333, 'epoch': 268.0}\n",
      "{'loss': 0.3636, 'learning_rate': 0.0008122222222222222, 'epoch': 269.0}\n",
      "{'loss': 0.3617, 'learning_rate': 0.0008111111111111111, 'epoch': 270.0}\n",
      "{'loss': 0.3639, 'learning_rate': 0.0008100000000000001, 'epoch': 271.0}\n",
      "{'loss': 0.366, 'learning_rate': 0.0008088888888888889, 'epoch': 272.0}\n",
      "{'loss': 0.3621, 'learning_rate': 0.0008077777777777778, 'epoch': 273.0}\n",
      "{'loss': 0.3656, 'learning_rate': 0.0008066666666666667, 'epoch': 274.0}\n",
      "{'loss': 0.3658, 'learning_rate': 0.0008055555555555556, 'epoch': 275.0}\n",
      "{'loss': 0.3655, 'learning_rate': 0.0008044444444444444, 'epoch': 276.0}\n",
      "{'loss': 0.3667, 'learning_rate': 0.0008033333333333333, 'epoch': 277.0}\n",
      "{'loss': 0.3624, 'learning_rate': 0.0008022222222222222, 'epoch': 278.0}\n",
      "{'loss': 0.3646, 'learning_rate': 0.0008011111111111112, 'epoch': 279.0}\n",
      "{'loss': 0.3653, 'learning_rate': 0.0008, 'epoch': 280.0}\n",
      "{'loss': 0.3692, 'learning_rate': 0.0007988888888888889, 'epoch': 281.0}\n",
      "{'loss': 0.3669, 'learning_rate': 0.0007977777777777778, 'epoch': 282.0}\n",
      "{'loss': 0.3646, 'learning_rate': 0.0007966666666666667, 'epoch': 283.0}\n",
      "{'loss': 0.3634, 'learning_rate': 0.0007955555555555555, 'epoch': 284.0}\n",
      "{'loss': 0.3647, 'learning_rate': 0.0007944444444444444, 'epoch': 285.0}\n",
      "{'loss': 0.3605, 'learning_rate': 0.0007933333333333334, 'epoch': 286.0}\n",
      "{'loss': 0.3639, 'learning_rate': 0.0007922222222222223, 'epoch': 287.0}\n",
      "{'loss': 0.3629, 'learning_rate': 0.0007911111111111111, 'epoch': 288.0}\n",
      "{'loss': 0.3637, 'learning_rate': 0.00079, 'epoch': 289.0}\n",
      "{'loss': 0.3641, 'learning_rate': 0.0007888888888888889, 'epoch': 290.0}\n",
      "{'loss': 0.3633, 'learning_rate': 0.0007877777777777779, 'epoch': 291.0}\n",
      "{'loss': 0.3635, 'learning_rate': 0.0007866666666666666, 'epoch': 292.0}\n",
      "{'loss': 0.365, 'learning_rate': 0.0007855555555555556, 'epoch': 293.0}\n",
      "{'loss': 0.3639, 'learning_rate': 0.0007844444444444445, 'epoch': 294.0}\n",
      "{'loss': 0.364, 'learning_rate': 0.0007833333333333334, 'epoch': 295.0}\n",
      "{'loss': 0.3606, 'learning_rate': 0.0007822222222222222, 'epoch': 296.0}\n",
      "{'loss': 0.3607, 'learning_rate': 0.0007811111111111111, 'epoch': 297.0}\n",
      "{'loss': 0.36, 'learning_rate': 0.0007800000000000001, 'epoch': 298.0}\n",
      "{'loss': 0.3634, 'learning_rate': 0.0007788888888888889, 'epoch': 299.0}\n",
      "{'loss': 0.3632, 'learning_rate': 0.0007777777777777778, 'epoch': 300.0}\n",
      "{'loss': 0.3617, 'learning_rate': 0.0007766666666666666, 'epoch': 301.0}\n",
      "{'loss': 0.3617, 'learning_rate': 0.0007755555555555556, 'epoch': 302.0}\n",
      "{'loss': 0.3609, 'learning_rate': 0.0007744444444444445, 'epoch': 303.0}\n",
      "{'loss': 0.3643, 'learning_rate': 0.0007733333333333333, 'epoch': 304.0}\n",
      "{'loss': 0.3621, 'learning_rate': 0.0007722222222222223, 'epoch': 305.0}\n",
      "{'loss': 0.3611, 'learning_rate': 0.0007711111111111111, 'epoch': 306.0}\n",
      "{'loss': 0.3625, 'learning_rate': 0.0007700000000000001, 'epoch': 307.0}\n",
      "{'loss': 0.3605, 'learning_rate': 0.0007688888888888888, 'epoch': 308.0}\n",
      "{'loss': 0.3617, 'learning_rate': 0.0007677777777777778, 'epoch': 309.0}\n",
      "{'loss': 0.3644, 'learning_rate': 0.0007666666666666667, 'epoch': 310.0}\n",
      "{'loss': 0.3631, 'learning_rate': 0.0007655555555555556, 'epoch': 311.0}\n",
      "{'loss': 0.3633, 'learning_rate': 0.0007644444444444445, 'epoch': 312.0}\n",
      "{'loss': 0.3609, 'learning_rate': 0.0007633333333333333, 'epoch': 313.0}\n",
      "{'loss': 0.36, 'learning_rate': 0.0007622222222222223, 'epoch': 314.0}\n",
      "{'loss': 0.361, 'learning_rate': 0.0007611111111111111, 'epoch': 315.0}\n",
      "{'loss': 0.3605, 'learning_rate': 0.00076, 'epoch': 316.0}\n",
      "{'loss': 0.36, 'learning_rate': 0.0007588888888888888, 'epoch': 317.0}\n",
      "{'loss': 0.3589, 'learning_rate': 0.0007577777777777778, 'epoch': 318.0}\n",
      "{'loss': 0.3611, 'learning_rate': 0.0007566666666666668, 'epoch': 319.0}\n",
      "{'loss': 0.3598, 'learning_rate': 0.0007555555555555555, 'epoch': 320.0}\n",
      "{'loss': 0.3617, 'learning_rate': 0.0007544444444444445, 'epoch': 321.0}\n",
      "{'loss': 0.3598, 'learning_rate': 0.0007533333333333333, 'epoch': 322.0}\n",
      "{'loss': 0.3593, 'learning_rate': 0.0007522222222222223, 'epoch': 323.0}\n",
      "{'loss': 0.3618, 'learning_rate': 0.000751111111111111, 'epoch': 324.0}\n",
      "{'loss': 0.3601, 'learning_rate': 0.00075, 'epoch': 325.0}\n",
      "{'loss': 0.3627, 'learning_rate': 0.000748888888888889, 'epoch': 326.0}\n",
      "{'loss': 0.359, 'learning_rate': 0.0007477777777777778, 'epoch': 327.0}\n",
      "{'loss': 0.36, 'learning_rate': 0.0007466666666666667, 'epoch': 328.0}\n",
      "{'loss': 0.3619, 'learning_rate': 0.0007455555555555555, 'epoch': 329.0}\n",
      "{'loss': 0.359, 'learning_rate': 0.0007444444444444445, 'epoch': 330.0}\n",
      "{'loss': 0.3597, 'learning_rate': 0.0007433333333333333, 'epoch': 331.0}\n",
      "{'loss': 0.36, 'learning_rate': 0.0007422222222222222, 'epoch': 332.0}\n",
      "{'loss': 0.3572, 'learning_rate': 0.0007411111111111112, 'epoch': 333.0}\n",
      "{'loss': 0.3571, 'learning_rate': 0.00074, 'epoch': 334.0}\n",
      "{'loss': 0.3596, 'learning_rate': 0.000738888888888889, 'epoch': 335.0}\n",
      "{'loss': 0.3579, 'learning_rate': 0.0007377777777777777, 'epoch': 336.0}\n",
      "{'loss': 0.3585, 'learning_rate': 0.0007366666666666667, 'epoch': 337.0}\n",
      "{'loss': 0.3606, 'learning_rate': 0.0007355555555555555, 'epoch': 338.0}\n",
      "{'loss': 0.3612, 'learning_rate': 0.0007344444444444445, 'epoch': 339.0}\n",
      "{'loss': 0.3572, 'learning_rate': 0.0007333333333333333, 'epoch': 340.0}\n",
      "{'loss': 0.3581, 'learning_rate': 0.0007322222222222222, 'epoch': 341.0}\n",
      "{'loss': 0.3587, 'learning_rate': 0.0007311111111111112, 'epoch': 342.0}\n",
      "{'loss': 0.359, 'learning_rate': 0.00073, 'epoch': 343.0}\n",
      "{'loss': 0.3612, 'learning_rate': 0.0007288888888888889, 'epoch': 344.0}\n",
      "{'loss': 0.3608, 'learning_rate': 0.0007277777777777777, 'epoch': 345.0}\n",
      "{'loss': 0.3601, 'learning_rate': 0.0007266666666666667, 'epoch': 346.0}\n",
      "{'loss': 0.3597, 'learning_rate': 0.0007255555555555556, 'epoch': 347.0}\n",
      "{'loss': 0.3595, 'learning_rate': 0.0007244444444444444, 'epoch': 348.0}\n",
      "{'loss': 0.359, 'learning_rate': 0.0007233333333333334, 'epoch': 349.0}\n",
      "{'loss': 0.3557, 'learning_rate': 0.0007222222222222222, 'epoch': 350.0}\n",
      "{'loss': 0.3584, 'learning_rate': 0.0007211111111111112, 'epoch': 351.0}\n",
      "{'loss': 0.3579, 'learning_rate': 0.0007199999999999999, 'epoch': 352.0}\n",
      "{'loss': 0.3595, 'learning_rate': 0.0007188888888888889, 'epoch': 353.0}\n",
      "{'loss': 0.359, 'learning_rate': 0.0007177777777777778, 'epoch': 354.0}\n",
      "{'loss': 0.3591, 'learning_rate': 0.0007166666666666667, 'epoch': 355.0}\n",
      "{'loss': 0.3586, 'learning_rate': 0.0007155555555555555, 'epoch': 356.0}\n",
      "{'loss': 0.3596, 'learning_rate': 0.0007144444444444444, 'epoch': 357.0}\n",
      "{'loss': 0.3598, 'learning_rate': 0.0007133333333333334, 'epoch': 358.0}\n",
      "{'loss': 0.3581, 'learning_rate': 0.0007122222222222222, 'epoch': 359.0}\n",
      "{'loss': 0.3592, 'learning_rate': 0.0007111111111111111, 'epoch': 360.0}\n",
      "{'loss': 0.3572, 'learning_rate': 0.00071, 'epoch': 361.0}\n",
      "{'loss': 0.3585, 'learning_rate': 0.0007088888888888889, 'epoch': 362.0}\n",
      "{'loss': 0.3573, 'learning_rate': 0.0007077777777777778, 'epoch': 363.0}\n",
      "{'loss': 0.3573, 'learning_rate': 0.0007066666666666666, 'epoch': 364.0}\n",
      "{'loss': 0.3585, 'learning_rate': 0.0007055555555555556, 'epoch': 365.0}\n",
      "{'loss': 0.3589, 'learning_rate': 0.0007044444444444445, 'epoch': 366.0}\n",
      "{'loss': 0.3564, 'learning_rate': 0.0007033333333333334, 'epoch': 367.0}\n",
      "{'loss': 0.3587, 'learning_rate': 0.0007022222222222222, 'epoch': 368.0}\n",
      "{'loss': 0.3579, 'learning_rate': 0.0007011111111111111, 'epoch': 369.0}\n",
      "{'loss': 0.3569, 'learning_rate': 0.0007, 'epoch': 370.0}\n",
      "{'loss': 0.3586, 'learning_rate': 0.0006988888888888889, 'epoch': 371.0}\n",
      "{'loss': 0.3564, 'learning_rate': 0.0006977777777777778, 'epoch': 372.0}\n",
      "{'loss': 0.3566, 'learning_rate': 0.0006966666666666667, 'epoch': 373.0}\n",
      "{'loss': 0.3595, 'learning_rate': 0.0006955555555555556, 'epoch': 374.0}\n",
      "{'loss': 0.3572, 'learning_rate': 0.0006944444444444445, 'epoch': 375.0}\n",
      "{'loss': 0.3577, 'learning_rate': 0.0006933333333333333, 'epoch': 376.0}\n",
      "{'loss': 0.3578, 'learning_rate': 0.0006922222222222222, 'epoch': 377.0}\n",
      "{'loss': 0.3571, 'learning_rate': 0.0006911111111111111, 'epoch': 378.0}\n",
      "{'loss': 0.3583, 'learning_rate': 0.00069, 'epoch': 379.0}\n",
      "{'loss': 0.3582, 'learning_rate': 0.000688888888888889, 'epoch': 380.0}\n",
      "{'loss': 0.3575, 'learning_rate': 0.0006877777777777778, 'epoch': 381.0}\n",
      "{'loss': 0.3572, 'learning_rate': 0.0006866666666666667, 'epoch': 382.0}\n",
      "{'loss': 0.3554, 'learning_rate': 0.0006855555555555556, 'epoch': 383.0}\n",
      "{'loss': 0.358, 'learning_rate': 0.0006844444444444444, 'epoch': 384.0}\n",
      "{'loss': 0.3577, 'learning_rate': 0.0006833333333333333, 'epoch': 385.0}\n",
      "{'loss': 0.3563, 'learning_rate': 0.0006822222222222222, 'epoch': 386.0}\n",
      "{'loss': 0.3568, 'learning_rate': 0.0006811111111111112, 'epoch': 387.0}\n",
      "{'loss': 0.3561, 'learning_rate': 0.00068, 'epoch': 388.0}\n",
      "{'loss': 0.3555, 'learning_rate': 0.0006788888888888889, 'epoch': 389.0}\n",
      "{'loss': 0.3583, 'learning_rate': 0.0006777777777777778, 'epoch': 390.0}\n",
      "{'loss': 0.3559, 'learning_rate': 0.0006766666666666667, 'epoch': 391.0}\n",
      "{'loss': 0.3568, 'learning_rate': 0.0006755555555555555, 'epoch': 392.0}\n",
      "{'loss': 0.3581, 'learning_rate': 0.0006744444444444444, 'epoch': 393.0}\n",
      "{'loss': 0.3562, 'learning_rate': 0.0006733333333333334, 'epoch': 394.0}\n",
      "{'loss': 0.356, 'learning_rate': 0.0006722222222222223, 'epoch': 395.0}\n",
      "{'loss': 0.3548, 'learning_rate': 0.0006711111111111111, 'epoch': 396.0}\n",
      "{'loss': 0.3573, 'learning_rate': 0.00067, 'epoch': 397.0}\n",
      "{'loss': 0.356, 'learning_rate': 0.0006688888888888889, 'epoch': 398.0}\n",
      "{'loss': 0.3563, 'learning_rate': 0.0006677777777777778, 'epoch': 399.0}\n",
      "{'loss': 0.3559, 'learning_rate': 0.0006666666666666666, 'epoch': 400.0}\n",
      "{'loss': 0.356, 'learning_rate': 0.0006655555555555556, 'epoch': 401.0}\n",
      "{'loss': 0.3569, 'learning_rate': 0.0006644444444444444, 'epoch': 402.0}\n",
      "{'loss': 0.3582, 'learning_rate': 0.0006633333333333334, 'epoch': 403.0}\n",
      "{'loss': 0.3552, 'learning_rate': 0.0006622222222222222, 'epoch': 404.0}\n",
      "{'loss': 0.3548, 'learning_rate': 0.0006611111111111111, 'epoch': 405.0}\n",
      "{'loss': 0.3559, 'learning_rate': 0.00066, 'epoch': 406.0}\n",
      "{'loss': 0.356, 'learning_rate': 0.0006588888888888889, 'epoch': 407.0}\n",
      "{'loss': 0.3573, 'learning_rate': 0.0006577777777777779, 'epoch': 408.0}\n",
      "{'loss': 0.3548, 'learning_rate': 0.0006566666666666666, 'epoch': 409.0}\n",
      "{'loss': 0.356, 'learning_rate': 0.0006555555555555556, 'epoch': 410.0}\n",
      "{'loss': 0.354, 'learning_rate': 0.0006544444444444445, 'epoch': 411.0}\n",
      "{'loss': 0.3556, 'learning_rate': 0.0006533333333333333, 'epoch': 412.0}\n",
      "{'loss': 0.3565, 'learning_rate': 0.0006522222222222222, 'epoch': 413.0}\n",
      "{'loss': 0.356, 'learning_rate': 0.0006511111111111111, 'epoch': 414.0}\n",
      "{'loss': 0.353, 'learning_rate': 0.0006500000000000001, 'epoch': 415.0}\n",
      "{'loss': 0.3549, 'learning_rate': 0.0006488888888888888, 'epoch': 416.0}\n",
      "{'loss': 0.3557, 'learning_rate': 0.0006477777777777778, 'epoch': 417.0}\n",
      "{'loss': 0.3547, 'learning_rate': 0.0006466666666666666, 'epoch': 418.0}\n",
      "{'loss': 0.3551, 'learning_rate': 0.0006455555555555556, 'epoch': 419.0}\n",
      "{'loss': 0.3554, 'learning_rate': 0.0006444444444444444, 'epoch': 420.0}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"decision_models/\",\n",
    "    logging_dir=\"Logs/DTRL/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=1000,\n",
    "    per_device_train_batch_size=60,\n",
    "    logging_strategy='epoch',\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=-1e-9,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    use_cpu=False,\n",
    "    max_grad_norm=0.1,\n",
    "    report_to='tensorboard'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=traj_dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# best_trial = trainer.hyperparameter_search(\n",
    "#     direction=\"maximize\", \n",
    "#     backend=\"ray\", \n",
    "#     n_trials=10 # number of trials\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TrainableDT(config).from_pretrained('decision_models/checkpoint-1300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = model.to(\"cpu\")\n",
    "env = env\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_ep_len = 67\n",
    "device = \"cpu\"\n",
    "scale = stats[\"return_max\"]+1  # normalization for rewards/returns\n",
    "TARGET_RETURN = 1  # evaluation is conditioned on a return of 320, scaled accordingly\n",
    "\n",
    "state_mean = collator.state_mean.astype(np.float32)\n",
    "state_std = collator.state_std.astype(np.float32)\n",
    "\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "# Create the decision transformer model\n",
    "\n",
    "state_mean = torch.from_numpy(state_mean).to(device=device)\n",
    "state_std = torch.from_numpy(state_std).to(device=device)\n",
    "act_dim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (34,), float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n",
    "    # This implementation does not condition on past rewards\n",
    "\n",
    "    states = states.reshape(1, -1, model.config.state_dim)\n",
    "    actions = actions.reshape(1, -1, model.config.act_dim)\n",
    "    returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
    "    timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "    states = states[:, -model.config.max_length :]\n",
    "    actions = actions[:, -model.config.max_length :]\n",
    "    returns_to_go = returns_to_go[:, -model.config.max_length :]\n",
    "    timesteps = timesteps[:, -model.config.max_length :]\n",
    "    padding = model.config.max_length - states.shape[1]\n",
    "    # pad all tokens to sequence length\n",
    "    attention_mask = torch.cat([torch.zeros(padding), torch.ones(states.shape[1])])\n",
    "    attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
    "    states = torch.cat([torch.zeros((1, padding, model.config.state_dim)), states], dim=1).float()\n",
    "    actions = torch.cat([torch.zeros((1, padding, model.config.act_dim)), actions], dim=1).float()\n",
    "    returns_to_go = torch.cat([torch.zeros((1, padding, 1)), returns_to_go], dim=1).float()\n",
    "    timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long), timesteps], dim=1)\n",
    "\n",
    "    state_preds, action_preds, return_preds = model.original_forward(\n",
    "        states=states,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        returns_to_go=returns_to_go,\n",
    "        timesteps=timesteps,\n",
    "        attention_mask=attention_mask,\n",
    "        return_dict=False,\n",
    "    )\n",
    "    # print(action_preds)\n",
    "    return action_preds[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=TournamentEnv(team_stats=team_data,\n",
    "                     season=2023,\n",
    "                     verbose=False,\n",
    "                     discrete=False,\n",
    "                     reward_on_round_end=False,\n",
    "                     shuffle=False,\n",
    "                     exclude_seasons=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'season': 2024,\n",
       " 'step': 63,\n",
       " 'max step': 63,\n",
       " 'num_correct': 63,\n",
       " 'round': '6',\n",
       " 'slot': 'R6CH',\n",
       " 'score': 318.0,\n",
       " 'round_score': 318.0}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interact with the environment and create a video\n",
    "\n",
    "# Interact with the environment and create a video\n",
    "episode_return, episode_length = 0, 0\n",
    "state,info = env.reset(season=2024)\n",
    "target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
    "states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
    "actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
    "rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "\n",
    "timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "done=False\n",
    "t=0\n",
    "while not done:\n",
    "    # print(t)\n",
    "    actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
    "    rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
    "\n",
    "    action = get_action(model,\n",
    "        (states - state_mean) / state_std,\n",
    "        actions,\n",
    "        rewards,\n",
    "        target_return,\n",
    "        timesteps,\n",
    "    )\n",
    "    actions[-1] = action\n",
    "    action = action.detach().cpu().numpy()\n",
    "    # print(action)\n",
    "    state, reward, done,truncated, info = env.step(action)\n",
    "    reward=1\n",
    "    cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
    "    states = torch.cat([states, cur_state], dim=0)\n",
    "    rewards[-1] = reward\n",
    "\n",
    "    pred_return = target_return[0, -1] - (reward / scale)\n",
    "    target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
    "    timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
    "\n",
    "    episode_return += 0\n",
    "    episode_length += 0\n",
    "    # t+=1\n",
    "\n",
    "info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: using box for unknown shape rounded\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
