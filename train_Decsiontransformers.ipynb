{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\IDrive-Sync\\MetaLocalLLC\\RL-Bots\\Tournament-gym\\tournamentgym.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tournamentgym import TournamentEnv\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO,A2C,SAC,DQN,DDPG\n",
    "\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.data.rollout import TrajectoryAccumulator\n",
    "from imitation.data.huggingface_utils import trajectories_to_dataset\n",
    "# from imitation.data import serialize\n",
    "from imitation.data.rollout import rollout_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3432cda61b4106835b55fb71b479c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MW='M'\n",
    "\n",
    "team_data=pd.read_csv(f'Process_data/{MW}_pm_encoded.csv')\n",
    "env=TournamentEnv(team_stats=team_data,\n",
    "                     season=2022,\n",
    "                     verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dems=1000\n",
    "traj_acum=TrajectoryAccumulator()\n",
    "demonstrations=[]\n",
    "for i in range(num_dems):\n",
    "    obs,info=env.reset()\n",
    "    terminated=False\n",
    "    step={'obs':obs}\n",
    "    traj_acum.add_step(key=i,step_dict=step)\n",
    "    j=0\n",
    "    while not terminated:\n",
    "        act=env.cheat_action() \n",
    "        j+=1\n",
    "        if i%10!=0:\n",
    "            if (j%20)==0:\n",
    "                act=np.random.choice([0,1])\n",
    "        next_obs, reward, terminated,truncated, info=env.step(action=act,)\n",
    "        \n",
    "        step={'obs':np.array(obs),\n",
    "            'acts':np.array([act]),\n",
    "            'rews':np.array(reward),\n",
    "            'infos':info}\n",
    "        traj_acum.add_step(key=i,\n",
    "                        step_dict=step,\n",
    "                        )\n",
    "        # print(reward,train_env.trade_week_start,train_env.current_time)\n",
    "        obs=next_obs\n",
    "    \n",
    "    \n",
    "    traj=traj_acum.finish_trajectory(key=i,terminal=terminated)\n",
    "    demonstrations.append(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_dataset=trajectories_to_dataset(demonstrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292.331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_traj': 1000,\n",
       " 'return_min': 173.5,\n",
       " 'return_mean': 292.331,\n",
       " 'return_std': 31.8413165399925,\n",
       " 'return_max': 320.0,\n",
       " 'len_min': 64,\n",
       " 'len_mean': 65.704,\n",
       " 'len_std': 1.4860632557196212,\n",
       " 'len_max': 67}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = rollout_stats(demonstrations)\n",
    "print(stats[\"return_mean\"])\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 20 #subsets of the episode we use for training\n",
    "    state_dim: int = 17  # size of state space\n",
    "\n",
    "    p_sample: np.array = None  \n",
    "    # n_traj: int = 0 \n",
    "\n",
    "    def __init__(self, demonstrations) -> None:\n",
    "        traj_stats=rollout_stats(demonstrations)\n",
    "        self.n_traj=traj_stats['n_traj'] # to store the number of trajectories in the dataset\n",
    "        self.max_ep_len=traj_stats['len_max'] # max episode length in the dataset\n",
    "        self.scale = traj_stats['return_max']  # normalization of rewards/returns\n",
    "        dataset=trajectories_to_dataset(demonstrations) ## turn list of trajectories into a dataset\n",
    "        self.dataset = dataset\n",
    "        self.act_dim = len(dataset[0]['acts'][0])\n",
    "        # calculate dataset stats for normalization of states\n",
    "        states = []\n",
    "        traj_lens = []\n",
    "        for obs in dataset[\"obs\"]:\n",
    "            obs=np.array(obs)\n",
    "            states.append(obs)\n",
    "            traj_lens.append(len(obs))\n",
    "        \n",
    "        states = np.vstack(states)\n",
    "        self.state_dim=states.shape[1]\n",
    "        self.state_mean = np.mean(states, axis=0) # to store state means\n",
    "\n",
    "        self.state_std=np.std(states, axis=0) + 1e-6 # to store state stds\n",
    "        traj_lens = np.array(traj_lens)\n",
    "        self.p_sample = traj_lens / sum(traj_lens) # a distribution to take account trajectory lengths\n",
    "\n",
    "    def _discount_cumsum(self, x, gamma):\n",
    "\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_size = len(features)\n",
    "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "        batch_inds = np.random.choice(\n",
    "            np.arange(self.n_traj),\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
    "        )\n",
    "        # a batch of dataset features\n",
    "        s, a, r, rtg, timesteps, mask = [], [], [], [], [], []\n",
    "        \n",
    "        for ind in batch_inds:\n",
    "            # for feature in features:\n",
    "            feature = self.dataset[int(ind)]\n",
    "            obs=np.array(feature[\"obs\"])\n",
    "            rews=np.array(feature[\"rews\"]).reshape(-1,1)\n",
    "            acts=np.array(feature[\"acts\"])\n",
    "\n",
    "            si = random.randint(1, len(rews)-1 )\n",
    "            \n",
    "\n",
    "            # get sequences from dataset\n",
    "            s.append(np.array(obs[si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
    "            a.append(np.array(acts[si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "            r.append(np.array(rews[si : si + self.max_len]).reshape(1, -1, 1))\n",
    "\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "            \n",
    "            try:\n",
    "                rews[si:]\n",
    "                discount_rew=self._discount_cumsum(np.array(rews[si:]), gamma=1.0)\n",
    "            except:\n",
    "                print(rews[si:])\n",
    "            discount_rew=self._discount_cumsum(np.array(rews[si:]), gamma=1.0)\n",
    "\n",
    "            slen=s[-1].shape[1]\n",
    "\n",
    "            rew_tg=discount_rew[: slen].reshape(1, -1, 1)\n",
    "            rtg.append(rew_tg)\n",
    "            rtg_len=rtg[-1].shape[1]\n",
    "            rtg_pad=np.zeros((1,self.max_len-rtg_len, 1))\n",
    "            rtg[-1] = np.concatenate([rtg[-1],rtg_pad], axis=1)\n",
    "            rtg[-1]=rtg[-1]/ self.scale\n",
    "\n",
    "            # padding and state + reward normalization\n",
    "            s_pad=np.zeros((1, self.max_len - slen, self.state_dim))\n",
    "            s[-1] = np.concatenate([s_pad, s[-1]], axis=1)\n",
    "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
    "\n",
    "            alen=a[-1].shape[1]\n",
    "            act_pad=np.ones((1, self.max_len - alen, self.act_dim))\n",
    "            a[-1] = np.concatenate([ act_pad* -10.0, a[-1]], axis=1,)\n",
    "\n",
    "            rlen=r[-1].shape[1]\n",
    "            rew_pad=np.zeros((1, self.max_len - rlen, 1))\n",
    "            r[-1] = np.concatenate([rew_pad, r[-1]], axis=1)\n",
    "\n",
    "            tlen=timesteps[-1].shape[1]\n",
    "            time_pad=np.zeros((1, self.max_len - tlen))\n",
    "            timesteps[-1] = np.concatenate([time_pad, timesteps[-1]], axis=1)\n",
    "\n",
    "            masked=np.zeros((1, self.max_len - alen))\n",
    "            unmasked=np.ones((1, alen))\n",
    "            mask.append(np.concatenate([masked,unmasked], axis=1))\n",
    "\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "        return {\n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"returns_to_go\": rtg,\n",
    "            \"timesteps\": timesteps,\n",
    "            \"attention_mask\": mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableDT(DecisionTransformerModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.soft=torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        output = super().forward(**kwargs)\n",
    "        # add the DT loss\n",
    "        action_preds = output[1]\n",
    "        action_targets = kwargs[\"actions\"]\n",
    "        attention_mask = kwargs[\"attention_mask\"]\n",
    "        act_dim = action_preds.shape[2]\n",
    "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
    "        action_preds=self.soft(action_preds)        \n",
    "        losses = torch.binary_cross_entropy_with_logits(action_preds ,action_targets) \n",
    "        loss=torch.mean(losses)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def original_forward(self, **kwargs):\n",
    "        return super().forward(**kwargs)\n",
    "    \n",
    "    def get_action(self, states, actions, rewards, returns_to_go, timesteps):\n",
    "        # This implementation does not condition on past rewards\n",
    "\n",
    "        states = states.reshape(1, -1, self.config.state_dim)\n",
    "        actions = actions.reshape(1, -1, self.config.act_dim)\n",
    "        returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
    "        timesteps = timesteps.reshape(1, -1)\n",
    "\n",
    "        states = states[:, -self.config.max_length :]\n",
    "        actions = actions[:, -self.config.max_length :]\n",
    "        returns_to_go = returns_to_go[:, -self.config.max_length :]\n",
    "        timesteps = timesteps[:, -self.config.max_length :]\n",
    "        padding = self.config.max_length - states.shape[1]\n",
    "        # pad all tokens to sequence length\n",
    "        attention_mask = torch.cat([torch.zeros(padding), torch.ones(states.shape[1])])\n",
    "        attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
    "        states = torch.cat([torch.zeros((1, padding, self.config.state_dim)), states], dim=1).float()\n",
    "        returns_to_go = torch.cat([torch.zeros((1, padding, 1)), returns_to_go], dim=1).float()\n",
    "        timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long), timesteps], dim=1)\n",
    "\n",
    "        state_preds, action_preds, return_preds = self.original_forward(\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            returns_to_go=returns_to_go,\n",
    "            timesteps=timesteps,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=False,\n",
    "        )\n",
    "        action_preds=self.soft(action_preds) \n",
    "        action=int(action_preds[0, -1])\n",
    "\n",
    "        return action\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "collator = DecisionTransformerGymDataCollator(demonstrations)\n",
    "\n",
    "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
    "model = TrainableDT(config)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0201bc90e29b4866bfe4fbb9505ab4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8484, 'learning_rate': 1e-05, 'epoch': 1.0}\n",
      "{'loss': 0.8481, 'learning_rate': 2e-05, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"decision_models/\",\n",
    "    logging_dir=\"Logs/DTRL/\",\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=60,\n",
    "    logging_strategy='epoch',\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    max_grad_norm=0.25,\n",
    "\n",
    "    report_to='tensorboard'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=traj_dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_trial = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\", \n",
    "    backend=\"ray\", \n",
    "    n_trials=10 # number of trials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(\"cpu\")\n",
    "env = env\n",
    "\n",
    "max_ep_len = 67\n",
    "device = \"cpu\"\n",
    "scale = 1000.0  # normalization for rewards/returns\n",
    "TARGET_RETURN = 600 / scale  # evaluation is conditioned on a return of 600, scaled accordingly\n",
    "\n",
    "state_mean = collator.state_mean.astype(np.float32)\n",
    "state_std = collator.state_std.astype(np.float32)\n",
    "print(state_mean)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "# Create the decision transformer model\n",
    "\n",
    "state_mean = torch.from_numpy(state_mean).to(device=device)\n",
    "state_std = torch.from_numpy(state_std).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=TournamentEnv(team_stats=team_data,\n",
    "                     season=2023,\n",
    "                     verbose=False,\n",
    "                     exclude_seasons=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interact with the environment and create a video\n",
    "episode_return, episode_length = 0, 0\n",
    "state = env.reset(season=2023)\n",
    "target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
    "states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
    "actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
    "rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
    "\n",
    "timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
    "done=False\n",
    "while not done:\n",
    "    actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
    "    rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
    "\n",
    "    action = model.get_action(\n",
    "        (states - state_mean) / state_std,\n",
    "        actions,\n",
    "        rewards,\n",
    "        target_return,\n",
    "        timesteps,\n",
    "    )\n",
    "    actions[-1] = action\n",
    "    action = action.detach().cpu().numpy()\n",
    "\n",
    "    state, reward, done,truncated, info = env.step(action)\n",
    "\n",
    "    cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
    "    states = torch.cat([states, cur_state], dim=0)\n",
    "    rewards[-1] = reward\n",
    "\n",
    "    pred_return = target_return[0, -1] - (reward / scale)\n",
    "    target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
    "    timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
    "\n",
    "    episode_return += reward\n",
    "    episode_length += 1\n",
    "\n",
    "info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
