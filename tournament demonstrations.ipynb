{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tournamentgym import TournamentEnv\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments\n",
    "torch.cuda.is_available()\n",
    "from tqdm .autonotebook import tqdm\n",
    "from imitation.algorithms import sqil,bc\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.data.rollout import TrajectoryAccumulator\n",
    "from imitation.data.huggingface_utils import trajectories_to_dataset\n",
    "# from imitation.data import serialize\n",
    "from imitation.data.rollout import rollout_stats\n",
    "from imitation.data import serialize\n",
    "from imitation.algorithms import sqil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MW='M'\n",
    "\n",
    "team_data=pd.read_csv(f'Process_data/{MW}_pm_w_names.csv')\n",
    "env=TournamentEnv(team_stats=team_data,\n",
    "                     season=2022,\n",
    "                     verbose=False,\n",
    "                     discrete=False,\n",
    "                     shuffle=True,\n",
    "                     loading_bar=False,\n",
    "                     exclude_seasons=[2023])\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.cheat_action() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb37f67d41cf4a4487479c9f22111496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m terminated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m step\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m'\u001b[39m:obs}\n\u001b[1;32m----> 8\u001b[0m traj_acum\u001b[38;5;241m.\u001b[39madd_step(key\u001b[38;5;241m=\u001b[39mi,step_dict\u001b[38;5;241m=\u001b[39mstep)\n\u001b[0;32m      9\u001b[0m j\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated:\n",
      "File \u001b[1;32mc:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\imitation\\data\\rollout.py:71\u001b[0m, in \u001b[0;36mTrajectoryAccumulator.add_step\u001b[1;34m(self, step_dict, key)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialise the trajectory accumulator.\"\"\"\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_trajectories \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_step\u001b[39m(\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     73\u001b[0m     step_dict: Mapping[\u001b[38;5;28mstr\u001b[39m, Union[types\u001b[38;5;241m.\u001b[39mObservation, Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]],\n\u001b[0;32m     74\u001b[0m     key: Hashable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     75\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add a single step to the partial trajectory identified by `key`.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    Generally a single step could correspond to, e.g., one environment managed\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m            with multiple partial trajectories.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_trajectories[key]\u001b[38;5;241m.\u001b[39mappend(step_dict)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_dems=1000\n",
    "traj_acum=TrajectoryAccumulator()\n",
    "demonstrations=[]\n",
    "for i in tqdm(range(num_dems)):\n",
    "    obs,info=env.reset()\n",
    "    terminated=False\n",
    "    step={'obs':obs}\n",
    "    traj_acum.add_step(key=i,step_dict=step)\n",
    "    j=0\n",
    "    while not terminated:\n",
    "        act=env.cheat_action() \n",
    "        j+=1\n",
    "        if i%10!=0:\n",
    "            if (j%10)==0:\n",
    "                act=env.action_space.sample()\n",
    "                \n",
    "        next_obs, reward, terminated,truncated, info=env.step(action=act,)\n",
    "        \n",
    "        step={'obs':torch.from_numpy(np.array(obs)).cpu(),\n",
    "            'acts':torch.from_numpy(act).cpu(),\n",
    "            'rews':torch.from_numpy(np.array(reward)).cpu(),\n",
    "            'infos':info}\n",
    "        traj_acum.add_step(key=i,\n",
    "                        step_dict=step,\n",
    "                        )\n",
    "        # print(reward,train_env.trade_week_start,train_env.current_time)\n",
    "        obs=next_obs\n",
    "    \n",
    "    \n",
    "    traj=traj_acum.finish_trajectory(key=i,terminal=terminated)\n",
    "    demonstrations.append(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats = rollout_stats(demonstrations)\n",
    "# print(stats[\"return_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.util.logger import HierarchicalLogger,make_output_format,configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "format=make_output_format(_format='tensorboard', log_dir='decision_models/', log_suffix='', max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlog=HierarchicalLogger(default_logger=configure('copy_models/', ('log','csv'))\n",
    "                    \n",
    "                                                     )\n",
    "hlog.output_formats.append(format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1147253batch [1:07:48, 275.52batch/s]"
     ]
    }
   ],
   "source": [
    "    \n",
    "bc_trainer = bc.BC(\n",
    "                    observation_space=env.observation_space,\n",
    "                    action_space=env.action_space,\n",
    "                    demonstrations=demonstrations,\n",
    "                    rng = np.random.default_rng(1),\n",
    "                    device=torch.device('cpu'),\n",
    "                    # custom_logger=configure(folder='decision_models/', format_strs='tensorboard')\n",
    "                    custom_logger=hlog\n",
    "                )\n",
    "# bc_trainer.policy\n",
    "bc_trainer.train(\n",
    "    n_epochs=1000,\n",
    "                #  n_batches=10000,\n",
    "                 log_interval=10000,\n",
    "                 progress_bar =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m reward, _ \u001b[38;5;241m=\u001b[39m evaluate_policy(bc_trainer\u001b[38;5;241m.\u001b[39mpolicy, env, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReward:\u001b[39m\u001b[38;5;124m\"\u001b[39m, reward)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'evaluate_policy' is not defined"
     ]
    }
   ],
   "source": [
    "reward, _ = evaluate_policy(bc_trainer.policy, env, 10)\n",
    "print(\"Reward:\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'team_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env\u001b[38;5;241m=\u001b[39mTournamentEnv(team_stats\u001b[38;5;241m=\u001b[39mteam_data,\n\u001b[0;32m      2\u001b[0m                      season\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2022\u001b[39m,\n\u001b[0;32m      3\u001b[0m                      verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m                      discrete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m                      loading_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m                      shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      7\u001b[0m                      )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'team_data' is not defined"
     ]
    }
   ],
   "source": [
    "env=TournamentEnv(team_stats=team_data,\n",
    "                     season=2022,\n",
    "                     verbose=False,\n",
    "                     discrete=False,\n",
    "                     loading_bar=False,\n",
    "                     shuffle=True\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs,info=env.reset(season=2023)\n",
    "terminated=False\n",
    "while not terminated:\n",
    "    action=bc_trainer.policy.predict(obs,deterministic=True)[0]\n",
    "    next_obs, reward, terminated,truncated, info=env.step(action=action)\n",
    "\n",
    "    obs=next_obs\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize.save('demonstrations/Mens_demonstrations.pkl', demonstrations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
