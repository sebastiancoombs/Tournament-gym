{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sebas\\IDrive-Sync\\MetaLocalLLC\\RL-Bots\\Tournament-gym\\tournamentgym.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from tournamentgym import TournamentEnv\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins.hparams import api as thp\n",
    "import tensorflow as tf\n",
    "from hyperopt  import hp, fmin , tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.data.rollout import TrajectoryAccumulator\n",
    "from imitation.data.huggingface_utils import trajectories_to_dataset\n",
    "# from imitation.data import serialize\n",
    "from imitation.data.rollout import rollout_stats\n",
    "from imitation.data import serialize\n",
    "from imitation.algorithms import sqil,bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MW='M'\n",
    "\n",
    "team_data=pd.read_csv(f'Process_data/{MW}_pm_encoded.csv')\n",
    "train_env=TournamentEnv(team_stats=team_data,\n",
    "                     season=2022,\n",
    "                     verbose=False,\n",
    "                     loading_bar=False,\n",
    "                     exclude_seasons=[2023])\n",
    "\n",
    "\n",
    "test_env=TournamentEnv(team_stats=team_data,\n",
    "                     season=2023,\n",
    "                     verbose=False,\n",
    "                     loading_bar=False,\n",
    "                     )\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<imitation.data.huggingface_utils.TrajectoryDatasetSequence at 0x2262d9a4990>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demonstrations=serialize.load('demonstrations/Mens_demonstrations.pkl')\n",
    "demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281.613\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stats = rollout_stats(demonstrations)\n",
    "print(stats[\"return_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'demonstrations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m demonstrations[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minfos\n",
      "\u001b[1;31mNameError\u001b[0m: name 'demonstrations' is not defined"
     ]
    }
   ],
   "source": [
    "demonstrations[0].infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "bc_trainer = bc.BC(\n",
    "                    observation_space=train_env.observation_space,\n",
    "                    action_space=train_env.action_space,\n",
    "                    demonstrations=demonstrations,\n",
    "                    rng = np.random.default_rng(0),\n",
    "                    # device='cuda'\n",
    "                )\n",
    "bc_trainer.train(\n",
    "    # n_epochs=1000,\n",
    "                 n_batches=100,\n",
    "                 log_interval=1,\n",
    "                 progress_bar =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA_gather)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bc_trainer\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# n_epochs=1000,\u001b[39;00m\n\u001b[0;32m      3\u001b[0m                  n_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      4\u001b[0m                  log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m      5\u001b[0m                  progress_bar \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\imitation\\algorithms\\bc.py:495\u001b[0m, in \u001b[0;36mBC.train\u001b[1;34m(self, n_epochs, n_batches, on_epoch_end, on_batch_end, log_interval, log_rollouts_venv, log_rollouts_n_episodes, progress_bar, reset_tensorboard)\u001b[0m\n\u001b[0;32m    490\u001b[0m obs_tensor \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mmap_maybe_dict(\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: util\u001b[38;5;241m.\u001b[39msafe_to_tensor(x, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m    492\u001b[0m     types\u001b[38;5;241m.\u001b[39mmaybe_unwrap_dictobs(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m    493\u001b[0m )\n\u001b[0;32m    494\u001b[0m acts \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39msafe_to_tensor(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macts\u001b[39m\u001b[38;5;124m\"\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 495\u001b[0m training_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_calculator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy, obs_tensor, acts)\n\u001b[0;32m    497\u001b[0m \u001b[38;5;66;03m# Renormalise the loss to be averaged over the whole\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;66;03m# batch size instead of the minibatch size.\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;66;03m# If there is an incomplete batch, its gradients will be\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;66;03m# smaller, which may be helpful for stability.\u001b[39;00m\n\u001b[0;32m    501\u001b[0m loss \u001b[38;5;241m=\u001b[39m training_metrics\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m*\u001b[39m minibatch_size \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n",
      "File \u001b[1;32mc:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\imitation\\algorithms\\bc.py:130\u001b[0m, in \u001b[0;36mBehaviorCloningLossCalculator.__call__\u001b[1;34m(self, policy, obs, acts)\u001b[0m\n\u001b[0;32m    126\u001b[0m acts \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39msafe_to_tensor(acts)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# policy.evaluate_actions's type signatures are incorrect.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# See https://github.com/DLR-RM/stable-baselines3/issues/1679\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m (_, log_prob, entropy) \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mevaluate_actions(\n\u001b[0;32m    131\u001b[0m     tensor_obs,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     acts,\n\u001b[0;32m    133\u001b[0m )\n\u001b[0;32m    134\u001b[0m prob_true_act \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mexp(log_prob)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    135\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m log_prob\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:700\u001b[0m, in \u001b[0;36mActorCriticPolicy.evaluate_actions\u001b[1;34m(self, obs, actions)\u001b[0m\n\u001b[0;32m    698\u001b[0m     latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor\u001b[38;5;241m.\u001b[39mforward_critic(vf_features)\n\u001b[0;32m    699\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_dist_from_latent(latent_pi)\n\u001b[1;32m--> 700\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n\u001b[0;32m    701\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[0;32m    702\u001b[0m entropy \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mentropy()\n",
      "File \u001b[1;32mc:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\distributions.py:292\u001b[0m, in \u001b[0;36mCategoricalDistribution.log_prob\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\torch\\distributions\\categorical.py:141\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    139\u001b[0m value, log_pmf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits)\n\u001b[0;32m    140\u001b[0m value \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_pmf\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, value)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA_gather)"
     ]
    }
   ],
   "source": [
    "bc_trainer.train(\n",
    "    # n_epochs=1000,\n",
    "                 n_batches=100,\n",
    "                 log_interval=1,\n",
    "                 progress_bar =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward, _ = evaluate_policy(bc_trainer.policy, test_env, 10)\n",
    "print(\"Reward:\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_arch(x):\n",
    "    options=dict(\n",
    "                     net_arch=[x, x])\n",
    "    return options\n",
    "def _get_train_freq(x,ep):\n",
    "    options=(x,ep)\n",
    "    return options\n",
    "\n",
    "search_space = { \n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\",0.001,  1.0),\n",
    "    'exploration_fraction': hp.uniform('exploration_fraction',0.001,  1.0),\n",
    "    \"gamma\": hp.uniform(\"gamma\",0,1),\n",
    "    \"tau\": hp.uniform(\"tau\",0,1),\n",
    "    'train_freq': _get_train_freq(hp.choice('train_steps',[c for c in range(2,250,2)]),\n",
    "                                  hp.choice('train_episode',['step','episode'])\n",
    "                                  ),\n",
    "    'buffer_size':hp.randint(\"buffer_size\",0,100),\n",
    "    'batch_size':hp.randint(\"batch_size\",0,100),\n",
    "    \"gradient_steps\": hp.randint(\"gradient_steps\",0,100),\n",
    "    \"policy_kwargs\": _get_arch(hp.choice(\"policy_kwargs\",[64,128,264])),\n",
    "\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params[\"tensorboard_log\"]='Tournament_hypers/'\n",
    "    params['device']='cuda'\n",
    "    sqil_trainer = sqil.SQIL(\n",
    "                        venv=train_env,\n",
    "                        demonstrations=demonstrations,\n",
    "                        policy=\"MlpPolicy\",\n",
    "                        rl_kwargs=params\n",
    "                        \n",
    "                            )\n",
    "    \n",
    "    bc_trainer = bc.BC(\n",
    "                        observation_space=train_env.observation_space,\n",
    "                        action_space=train_env.action_space,\n",
    "                        demonstrations=demonstrations,\n",
    "                        rng = np.random.default_rng(0),\n",
    "                    )\n",
    "    bc_trainer.train(total_timesteps=70_000,tb_log_name='Tourny_maker')\n",
    "    # sqil_trainer.train(total_timesteps=70_000,tb_log_name='Tourny_maker')\n",
    "    obs,info=test_env.reset(season=2023)\n",
    "    terminated=False\n",
    "    while not terminated:\n",
    "        action=sqil_trainer.policy.predict(obs)[0]\n",
    "        next_obs, reward, terminated,truncated, info=test_env.step(action=action)\n",
    "        \n",
    "        obs=next_obs\n",
    "\n",
    "\n",
    "    \n",
    "    return -info['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = fmin(objective, search_space, algo=tpe.suggest, max_evals=100)\n",
    "best[\"tensorboard_log\"]='Tournament_hypers/'\n",
    "best['device']='cuda'\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqil_trainer = sqil.SQIL(\n",
    "                        venv=copy.deepcopy(train_env),\n",
    "                        demonstrations=demonstrations,\n",
    "                        **best\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_before_training, _ = evaluate_policy(sqil_trainer.policy, sqil_trainer.venv, 2)\n",
    "print(\"Reward before training:\", reward_before_training)\n",
    "\n",
    "sqil_trainer.train(total_timesteps=1_000_000,tb_log_name='Trader')\n",
    "reward_after_training, _ = evaluate_policy(sqil_trainer.policy, sqil_trainer.venv, 2)\n",
    "print(\"Reward after training:\", reward_after_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs,info=test_env.reset(season=2023)\n",
    "terminated=False\n",
    "while not terminated:\n",
    "    action=sqil_trainer.policy.predict(obs)[0]\n",
    "    next_obs, reward, terminated,truncated, info=test_env.step(action=action,cheat=False)\n",
    "\n",
    "    obs=next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
