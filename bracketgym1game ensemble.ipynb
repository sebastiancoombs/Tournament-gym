{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bracketology_rl import Bracket\n",
    "from bracketgym1game import TournamentEnv\n",
    "import warnings\n",
    "import gym\n",
    "import numpy as np\n",
    "from tourney_tools import * \n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing as mp\n",
    "import traceback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO,A2C,SAC,DQN,DDPG\n",
    "from sb3_contrib import RecurrentPPO,ARS,QRDQN\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "## tensor board_log\n",
    "tensorboard_log = \"Logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2003,2019))\n",
    "year=years[1]\n",
    "\n",
    "bracket = Bracket(2023)\n",
    "\n",
    "team_data=pd.read_csv('Process_data/pm_w_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=TournamentEnv(team_data=team_data,weight_scores=True,discrete=False,loading_bar=True)\n",
    "ensemble=[A2C,ARS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_ars_model(model,env):\n",
    "    policies=[p for p in model.policy_aliases]\n",
    "    pol=policies[0]\n",
    "    print(policies,policies[0])\n",
    "    model = model(policy=pol, env=env, verbose=0,\n",
    "    learning_rate= .001,\n",
    "    # gamma=  0.99,\n",
    "\n",
    "    # exploration_fraction=1,\n",
    "    tensorboard_log='Logs/')\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=TournamentEnv(team_data=team_data,weight_scores=True,discrete=False,loading_bar=False)\n",
    "models=[]\n",
    "# for model in ensembleQs:\n",
    "#     models.append(make_Q_model(model,env) )\n",
    "for model in ensemble:\n",
    "    models.append(make_ars_model(model,env) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Models ->2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params=[]\n",
    "bar=tqdm(range(len(models)))\n",
    "\n",
    "for i,model in enumerate(models):\n",
    "    bar.update(1)\n",
    "    try:\n",
    "        stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=1000, min_evals=1000, verbose=0)\n",
    "        eval_callback = EvalCallback(model.env, eval_freq=20, callback_after_eval=stop_train_callback, verbose=0)\n",
    "        model.learn(5000,callback=eval_callback)\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the ensembled models on 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=TournamentEnv(team_data=team_data,discrete=False,weight_scores=False,loading_bar=True,bust_stop=False)\n",
    "## test as an ensemble\n",
    "scores=[]\n",
    "\n",
    "for model in models:\n",
    "    model.set_env(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test one 2022\n",
    "obs = env.reset(season=2022,weight_scores=False,track_wins=True)\n",
    "done = False\n",
    "while not done:\n",
    "    actions = [model.predict(obs,deterministic =True)[0] for model in models]\n",
    "    action=np.mean(actions, axis=0)\n",
    "\n",
    "    obs, reward, done, info = env.step(action)\n",
    "display(info['total_score'],info['num_correct'])    \n",
    "env.render(score=f\"Ensembled ARS Correct:{info['num_correct']} {info['total_score']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test individually\n",
    "scores=[]\n",
    "max_scores=[]\n",
    "model_scores={}\n",
    "dfs={}\n",
    "for i,model in enumerate(models):\n",
    "\n",
    "    obs = env.reset(season=2022,weight_scores=False,track_wins=False)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = model.predict(obs,deterministic=False)[0]\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "    total_score=info['total_score']\n",
    "    num_correct=info['num_correct']\n",
    "    scores.append(total_score) \n",
    "    dfs[total_score]=env.action_data\n",
    "    model_scores[total_score]=model\n",
    "    name=model.__repr__().split(' ')[0].split('.')[-1]\n",
    "    env.render(score=f'{name} {i} Correct:{num_correct} Score:{total_score}')\n",
    "display(info['total_score'],info['num_correct'])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick out the best Models\n",
    "scores=sorted(scores)\n",
    "topscore=scores[-1]\n",
    "secondscore=scores[-3]\n",
    "print(scores)\n",
    "top_model=model_scores[topscore]\n",
    "second_model=model_scores[secondscore]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict This year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs = env.reset(season=2023,weight_scores=False,track_wins=True)\n",
    "done = False\n",
    "while not done:\n",
    "    actions = [model.predict(obs,deterministic =False)[0] for model in models]\n",
    "    action=np.mean(actions, axis=0)\n",
    "\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "env.render(score=f\"Ensembled ARS Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs = env.reset(season=2023,weight_scores=False,track_wins=False)\n",
    "done = False\n",
    "while not done:\n",
    "    action = top_model.predict(obs,deterministic=False)[0]\n",
    "    obs, reward, done, info = env.step(action)\n",
    "name=top_model.__repr__().split(' ')[0].split('.')[-1]\n",
    "env.render(score=f'Top {name}  Pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(season=2023,weight_scores=False,track_wins=False)\n",
    "done = False\n",
    "while not done:\n",
    "    action = second_model.predict(obs,deterministic=False)[0]\n",
    "    obs, reward, done, info = env.step(action)\n",
    "name=second_model.__repr__().split(' ')[0].split('.')[-1]\n",
    "env.render(score=f'Second Best {name} Pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,model in enumerate(models):\n",
    "\n",
    "#     obs = env.reset(season=2023,weight_scores=False,track_wins=False)\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         action = model.predict(obs,deterministic=True)[0]\n",
    "\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#     env.render(score=f'Model {i} Pred')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jgfulfgu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c02ce77fc9854b14bbf737e9ff58b840b53c17f549f3baecfcf54a3b4065de24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
