{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bracketology_rl import Bracket\n",
    "from bracketgym1game import TournamentEnv\n",
    "import warnings\n",
    "import gym\n",
    "import numpy as np\n",
    "from tourney_tools import * \n",
    "warnings.filterwarnings('ignore')\n",
    "import multiprocessing as mp\n",
    "import traceback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO,A2C,SAC,DQN,DDPG\n",
    "from sb3_contrib import RecurrentPPO,ARS,QRDQN\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "## tensor board_log\n",
    "tensorboard_log = \"Logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2003,2019))\n",
    "year=years[1]\n",
    "\n",
    "bracket = Bracket(2023)\n",
    "\n",
    "team_data=pd.read_csv('Process_data/pm_w_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b535da44978d4c9ba197df3723479071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env=TournamentEnv(team_data=team_data,weight_scores=True,discrete=False,loading_bar=True)\n",
    "ensemble=[A2C,ARS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_ars_model(model,env):\n",
    "    policies=[p for p in model.policy_aliases]\n",
    "    pol=policies[0]\n",
    "    print(policies,policies[0])\n",
    "    model = model(policy=pol, env=env, verbose=0,\n",
    "    learning_rate= .001,\n",
    "    # gamma=  0.99,\n",
    "\n",
    "    # exploration_fraction=1,\n",
    "    tensorboard_log='Logs/')\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MlpPolicy', 'CnnPolicy', 'MultiInputPolicy'] MlpPolicy\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Calling torch.geqrf on a CPU tensor requires compiling PyTorch with LAPACK. Please use PyTorch built with LAPACK support.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# for model in ensembleQs:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     models.append(make_Q_model(model,env) )\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m ensemble:\n\u001b[1;32m----> 6\u001b[0m     models\u001b[38;5;241m.\u001b[39mappend(make_ars_model(model,env) )\n",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m, in \u001b[0;36mmake_ars_model\u001b[1;34m(model, env)\u001b[0m\n\u001b[0;32m      3\u001b[0m pol\u001b[38;5;241m=\u001b[39mpolicies[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(policies,policies[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m model(policy\u001b[38;5;241m=\u001b[39mpol, env\u001b[38;5;241m=\u001b[39menv, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m      6\u001b[0m learning_rate\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.001\u001b[39m,\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# gamma=  0.99,\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# exploration_fraction=1,\u001b[39;00m\n\u001b[0;32m     10\u001b[0m tensorboard_log\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogs/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:123\u001b[0m, in \u001b[0;36mA2C.__init__\u001b[1;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, rms_prop_eps, use_rms_prop, use_sde, sde_sample_freq, normalize_advantage, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, eps\u001b[38;5;241m=\u001b[39mrms_prop_eps, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init_setup_model:\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_model()\n",
      "File \u001b[1;32mc:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:123\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer \u001b[38;5;241m=\u001b[39m buffer_cls(\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps,\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m     n_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs,\n\u001b[0;32m    121\u001b[0m )\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# pytype:disable=not-instantiable\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_class(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_schedule, use_sde\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_kwargs\n\u001b[0;32m    125\u001b[0m )\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# pytype:enable=not-instantiable\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:507\u001b[0m, in \u001b[0;36mActorCriticPolicy.__init__\u001b[1;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, ortho_init, use_sde, log_std_init, full_std, use_expln, squash_output, features_extractor_class, features_extractor_kwargs, share_features_extractor, normalize_images, optimizer_class, optimizer_kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# Action distribution\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist \u001b[38;5;241m=\u001b[39m make_proba_distribution(action_space, use_sde\u001b[38;5;241m=\u001b[39muse_sde, dist_kwargs\u001b[38;5;241m=\u001b[39mdist_kwargs)\n\u001b[1;32m--> 507\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build(lr_schedule)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:603\u001b[0m, in \u001b[0;36mActorCriticPolicy._build\u001b[1;34m(self, lr_schedule)\u001b[0m\n\u001b[0;32m    600\u001b[0m         module_gains[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf_features_extractor] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module, gain \u001b[38;5;129;01min\u001b[39;00m module_gains\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 603\u001b[0m         module\u001b[38;5;241m.\u001b[39mapply(partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_weights, gain\u001b[38;5;241m=\u001b[39mgain))\n\u001b[0;32m    605\u001b[0m \u001b[38;5;66;03m# Setup optimizer with initial learning rate\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_class(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr_schedule(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:884\u001b[0m, in \u001b[0;36mModule.apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;124;03mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;124;03m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m \n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 884\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m    885\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:884\u001b[0m, in \u001b[0;36mModule.apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\u001b[39;00m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;124;03mas well as self. Typical use includes initializing the parameters of a model\u001b[39;00m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;124;03m(see also :ref:`nn-init-doc`).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    881\u001b[0m \n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 884\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[0;32m    885\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:885\u001b[0m, in \u001b[0;36mModule.apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m    884\u001b[0m     module\u001b[38;5;241m.\u001b[39mapply(fn)\n\u001b[1;32m--> 885\u001b[0m fn(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:306\u001b[0m, in \u001b[0;36mBasePolicy.init_weights\u001b[1;34m(module, gain)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;124;03mOrthogonal initialization (used in PPO and A2C)\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, (nn\u001b[38;5;241m.\u001b[39mLinear, nn\u001b[38;5;241m.\u001b[39mConv2d)):\n\u001b[1;32m--> 306\u001b[0m     nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39morthogonal_(module\u001b[38;5;241m.\u001b[39mweight, gain\u001b[38;5;241m=\u001b[39mgain)\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m         module\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m0.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\anaconda3\\Lib\\site-packages\\torch\\nn\\init.py:484\u001b[0m, in \u001b[0;36morthogonal_\u001b[1;34m(tensor, gain)\u001b[0m\n\u001b[0;32m    481\u001b[0m     flattened\u001b[38;5;241m.\u001b[39mt_()\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Compute the qr factorization\u001b[39;00m\n\u001b[1;32m--> 484\u001b[0m q, r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mqr(flattened)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\u001b[39;00m\n\u001b[0;32m    486\u001b[0m d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(r, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Calling torch.geqrf on a CPU tensor requires compiling PyTorch with LAPACK. Please use PyTorch built with LAPACK support."
     ]
    }
   ],
   "source": [
    "env=TournamentEnv(team_data=team_data,weight_scores=True,discrete=False,loading_bar=False)\n",
    "models=[]\n",
    "# for model in ensembleQs:\n",
    "#     models.append(make_Q_model(model,env) )\n",
    "for model in ensemble:\n",
    "    models.append(make_ars_model(model,env) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Models ->2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params=[]\n",
    "bar=tqdm(range(len(models)))\n",
    "\n",
    "for i,model in enumerate(models):\n",
    "    bar.update(1)\n",
    "    try:\n",
    "        stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=1000, min_evals=1000, verbose=0)\n",
    "        eval_callback = EvalCallback(model.env, eval_freq=20, callback_after_eval=stop_train_callback, verbose=0)\n",
    "        model.learn(5000,callback=eval_callback)\n",
    "    except KeyboardInterrupt:\n",
    "\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test the ensembled models on 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=TournamentEnv(team_data=team_data,discrete=False,weight_scores=False,loading_bar=True,bust_stop=False)\n",
    "## test as an ensemble\n",
    "scores=[]\n",
    "\n",
    "for model in models:\n",
    "    model.set_env(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test one 2022\n",
    "obs = env.reset(season=2022,weight_scores=False,track_wins=True)\n",
    "done = False\n",
    "while not done:\n",
    "    actions = [model.predict(obs,deterministic =True)[0] for model in models]\n",
    "    action=np.mean(actions, axis=0)\n",
    "\n",
    "    obs, reward, done, info = env.step(action)\n",
    "display(info['total_score'],info['num_correct'])    \n",
    "env.render(score=f\"Ensembled ARS Correct:{info['num_correct']} {info['total_score']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test individually\n",
    "scores=[]\n",
    "max_scores=[]\n",
    "model_scores={}\n",
    "dfs={}\n",
    "for i,model in enumerate(models):\n",
    "\n",
    "    obs = env.reset(season=2022,weight_scores=False,track_wins=False)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = model.predict(obs,deterministic=False)[0]\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "    total_score=info['total_score']\n",
    "    num_correct=info['num_correct']\n",
    "    scores.append(total_score) \n",
    "    dfs[total_score]=env.action_data\n",
    "    model_scores[total_score]=model\n",
    "    name=model.__repr__().split(' ')[0].split('.')[-1]\n",
    "    env.render(score=f'{name} {i} Correct:{num_correct} Score:{total_score}')\n",
    "display(info['total_score'],info['num_correct'])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick out the best Models\n",
    "scores=sorted(scores)\n",
    "topscore=scores[-1]\n",
    "secondscore=scores[-3]\n",
    "print(scores)\n",
    "top_model=model_scores[topscore]\n",
    "second_model=model_scores[secondscore]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict This year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs = env.reset(season=2023,weight_scores=False,track_wins=True)\n",
    "done = False\n",
    "while not done:\n",
    "    actions = [model.predict(obs,deterministic =False)[0] for model in models]\n",
    "    action=np.mean(actions, axis=0)\n",
    "\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "env.render(score=f\"Ensembled ARS Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs = env.reset(season=2023,weight_scores=False,track_wins=False)\n",
    "done = False\n",
    "while not done:\n",
    "    action = top_model.predict(obs,deterministic=False)[0]\n",
    "    obs, reward, done, info = env.step(action)\n",
    "name=top_model.__repr__().split(' ')[0].split('.')[-1]\n",
    "env.render(score=f'Top {name}  Pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(season=2023,weight_scores=False,track_wins=False)\n",
    "done = False\n",
    "while not done:\n",
    "    action = second_model.predict(obs,deterministic=False)[0]\n",
    "    obs, reward, done, info = env.step(action)\n",
    "name=second_model.__repr__().split(' ')[0].split('.')[-1]\n",
    "env.render(score=f'Second Best {name} Pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,model in enumerate(models):\n",
    "\n",
    "#     obs = env.reset(season=2023,weight_scores=False,track_wins=False)\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         action = model.predict(obs,deterministic=True)[0]\n",
    "\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#     env.render(score=f'Model {i} Pred')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jgfulfgu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c02ce77fc9854b14bbf737e9ff58b840b53c17f549f3baecfcf54a3b4065de24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
