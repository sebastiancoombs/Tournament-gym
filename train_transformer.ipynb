{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from bracketology_rl import Bracket\n",
    "from bracketgym1game import TournamentEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO,A2C,SAC,DQN,DDPG\n",
    "from sb3_contrib import RecurrentPPO,ARS,QRDQN\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.data.rollout import TrajectoryAccumulator\n",
    "from imitation.data import serialize\n",
    "from imitation.data.rollout import rollout_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2003,2019))\n",
    "year=years[1]\n",
    "\n",
    "bracket = Bracket(2019)\n",
    "# bracket.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97cb9caa601442d99d392412680fec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "team_data=pd.read_csv('Process_data/pm_w_names.csv')\n",
    "env=TournamentEnv(team_data=team_data,weight_scores=True,discrete=True,loading_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 1\n",
      "2 0 1\n",
      "3 0 1\n",
      "4 0 1\n",
      "5 0 1\n",
      "6 0 1\n",
      "7 0 1\n",
      "8 0 1\n",
      "9 0 1\n",
      "10 0 1\n",
      "11 0 1\n",
      "12 0 1\n",
      "13 0 1\n",
      "14 0 1\n",
      "15 0 1\n",
      "16 0 1\n",
      "17 0 1\n",
      "18 0 1\n",
      "19 0 1\n",
      "20 1 1\n",
      "21 0 1\n",
      "22 0 1\n",
      "23 0 1\n",
      "24 0 1\n",
      "25 0 1\n",
      "26 0 1\n",
      "27 0 1\n",
      "28 0 1\n",
      "29 0 1\n",
      "30 1 1\n",
      "31 0 1\n",
      "32 0 1\n",
      "33 0 2\n",
      "34 1 2\n",
      "35 0 2\n",
      "36 1 2\n",
      "37 0 2\n",
      "38 1 2\n",
      "39 0 2\n",
      "40 0 2\n",
      "41 0 2\n",
      "42 0 2\n",
      "43 1 2\n",
      "44 0 2\n",
      "45 0 2\n",
      "['Wisconsin', 'Kansas St']\n",
      "['Duke', 'Gonzaga', 'North Carolina', 'Virginia', 'Kentucky', 'Michigan', 'Michigan St', 'Tennessee', 'Houston', 'LSU', 'Purdue', 'Texas Tech', 'Florida St', 'Kansas', 'Virginia Tech', 'Auburn', 'Buffalo', 'Maryland', 'Villanova', 'Wofford', 'Baylor', 'Oklahoma', 'UCF', 'Washington', 'Florida', 'Iowa', 'Minnesota', 'Ohio St', 'Liberty', 'Murray St', 'Oregon', 'UC Irvine']\n",
      "32\n",
      "46 ['oops'] 2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     act\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(i,act,env\u001b[38;5;241m.\u001b[39mcurrent_round)\n\u001b[1;32m---> 11\u001b[0m next_obs, reward, done,truncated, info\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m=\u001b[39mact)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\IDrive-Sync\\MetaLocalLLC\\RL-Bots\\Tournament-gym\\bracketgym1game.py:353\u001b[0m, in \u001b[0;36mTournamentEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    351\u001b[0m done\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    352\u001b[0m truncated\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_actions(action)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m## chose the winning team\u001b[39;00m\n\u001b[0;32m    355\u001b[0m winning_team\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoose_team(action)\n",
      "File \u001b[1;32mc:\\Users\\sebas\\IDrive-Sync\\MetaLocalLLC\\RL-Bots\\Tournament-gym\\bracketgym1game.py:343\u001b[0m, in \u001b[0;36mTournamentEnv.get_actions\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscrete:\n\u001b[0;32m    342\u001b[0m     actions\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 343\u001b[0m     actions[action]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    344\u001b[0m     action\u001b[38;5;241m=\u001b[39mactions\n\u001b[0;32m    345\u001b[0m actions\u001b[38;5;241m=\u001b[39maction\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "done=False\n",
    "i=0\n",
    "env.reset()\n",
    "while not done:\n",
    "    i+=1\n",
    "    act=env.get_correct_action()\n",
    "\n",
    "    if (i%10)==0:\n",
    "        act=int(np.random.choice([0,1]))\n",
    "    print(i,act,env.current_round)\n",
    "    next_obs, reward, done,truncated, info=env.step(action=act)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dems=500\n",
    "traj_acum=TrajectoryAccumulator()\n",
    "demonstrations=[]\n",
    "j=0\n",
    "for i in range(num_dems):\n",
    "    obs,info=env.reset()\n",
    "\n",
    "    terminated=False\n",
    "    step={'obs':obs}\n",
    "    traj_acum.add_step(key=i,step_dict=step)\n",
    "    while not terminated:\n",
    "        act=env.get_correct_action()\n",
    "        if (j%40)==0:\n",
    "            act=np.random.choice([0,1])\n",
    "        next_obs, reward, terminated,truncated, info=env.step(action=act,)\n",
    "        \n",
    "        step={'obs':obs,\n",
    "            'acts':act,\n",
    "            'rews':np.array(reward),\n",
    "            'infos':info}\n",
    "        traj_acum.add_step(key=i,\n",
    "                        step_dict=step,\n",
    "                        )\n",
    "        # print(reward,train_env.trade_week_start,train_env.current_time)\n",
    "        obs=next_obs\n",
    "    \n",
    "    traj=traj_acum.finish_trajectory(key=i,terminal=terminated)\n",
    "    demonstrations.append(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = rollout_stats(demonstrations)\n",
    "print(stats[\"return_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DecisionTransformerGymDataCollator:\n",
    "    return_tensors: str = \"pt\"\n",
    "    max_len: int = 20 #subsets of the episode we use for training\n",
    "    state_dim: int = 17  # size of state space\n",
    "    act_dim: int = 6  # size of action space\n",
    "    max_ep_len: int = 63 # max episode length in the dataset\n",
    "    scale: float = 1000.0  # normalization of rewards/returns\n",
    "    state_mean: np.array = None  # to store state means\n",
    "    state_std: np.array = None  # to store state stds\n",
    "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
    "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
    "\n",
    "    def __init__(self, dataset) -> None:\n",
    "        self.act_dim = len(dataset[0][\"actions\"][0])\n",
    "        self.state_dim = len(dataset[0][\"observations\"][0])\n",
    "        self.dataset = dataset\n",
    "        # calculate dataset stats for normalization of states\n",
    "        states = []\n",
    "        traj_lens = []\n",
    "        for obs in dataset[\"observations\"]:\n",
    "            states.extend(obs)\n",
    "            traj_lens.append(len(obs))\n",
    "        self.n_traj = len(traj_lens)\n",
    "        states = np.vstack(states)\n",
    "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "        \n",
    "        traj_lens = np.array(traj_lens)\n",
    "        self.p_sample = traj_lens / sum(traj_lens)\n",
    "\n",
    "    def _discount_cumsum(self, x, gamma):\n",
    "        discount_cumsum = np.zeros_like(x)\n",
    "        discount_cumsum[-1] = x[-1]\n",
    "        for t in reversed(range(x.shape[0] - 1)):\n",
    "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "        return discount_cumsum\n",
    "\n",
    "    def __call__(self, features):\n",
    "        batch_size = len(features)\n",
    "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
    "        batch_inds = np.random.choice(\n",
    "            np.arange(self.n_traj),\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
    "        )\n",
    "        # a batch of dataset features\n",
    "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "        \n",
    "        for ind in batch_inds:\n",
    "            # for feature in features:\n",
    "            feature = self.dataset[int(ind)]\n",
    "            si = random.randint(0, len(feature[\"rewards\"]) - 1)\n",
    "\n",
    "            # get sequences from dataset\n",
    "            s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
    "            a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
    "            r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
    "\n",
    "            d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
    "            rtg.append(\n",
    "                self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
    "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
    "                ].reshape(1, -1, 1)\n",
    "            )\n",
    "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
    "                print(\"if true\")\n",
    "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "            # padding and state + reward normalization\n",
    "            tlen = s[-1].shape[1]\n",
    "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
    "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
    "            a[-1] = np.concatenate(\n",
    "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
    "                axis=1,\n",
    "            )\n",
    "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
    "            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
    "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
    "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
    "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
    "\n",
    "        return {\n",
    "            \"states\": s,\n",
    "            \"actions\": a,\n",
    "            \"rewards\": r,\n",
    "            \"returns_to_go\": rtg,\n",
    "            \"timesteps\": timesteps,\n",
    "            \"attention_mask\": mask,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
