{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from tournamentgym import TournamentEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pearl.pearl.pearl_agent import PearlAgent\n",
    "from Pearl.pearl.action_representation_modules.one_hot_action_representation_module import (\n",
    "    OneHotActionTensorRepresentationModule,\n",
    ")\n",
    "from Pearl.pearl.policy_learners.sequential_decision_making.deep_q_learning import (\n",
    "    DeepQLearning,\n",
    ")\n",
    "from Pearl.pearl.replay_buffers.sequential_decision_making.fifo_off_policy_replay_buffer import (\n",
    "    FIFOOffPolicyReplayBuffer,\n",
    ")\n",
    "from Pearl.pearl.utils.instantiations.environments.gym_environment import GymEnvironment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pearl.pearl.neural_networks.sequential_decision_making.q_value_networks import VanillaQValueNetwork\n",
    "from Pearl.pearl.utils.functional_utils.experimentation.set_seed import set_seed\n",
    "from Pearl.pearl.policy_learners.sequential_decision_making.deep_q_learning import DeepQLearning\n",
    "from Pearl.pearl.policy_learners.sequential_decision_making.double_dqn import DoubleDQN\n",
    "from Pearl.pearl.replay_buffers.sequential_decision_making.fifo_off_policy_replay_buffer import FIFOOffPolicyReplayBuffer\n",
    "from Pearl.pearl.utils.functional_utils.train_and_eval.online_learning import online_learning\n",
    "from Pearl.pearl.pearl_agent import PearlAgent\n",
    "from Pearl.pearl.utils.instantiations.environments.gym_environment import GymEnvironment\n",
    "from Pearl.pearl.action_representation_modules.one_hot_action_representation_module import (\n",
    "    OneHotActionTensorRepresentationModule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pearl.utils.instantiations.spaces.discrete_action.DiscreteActionSpace at 0x2929da9bd90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "team_data=pd.read_csv('Process_data/M_pm_w_names.csv')\n",
    "env=TournamentEnv(season=2022,team_stats=team_data,discrete=True,shuffle=True,exclude_seasons=[2023,2024],reward_on_round_end=False,loading_bar=False)\n",
    "env=GymEnvironment(env)\n",
    "num_actions = env.action_space.n\n",
    "# VanillaQValueNetwork class uses a simple mlp for approximating the Q values.\n",
    "#  - Input dimension of the mlp = (state_dim + action_dim)\n",
    "#  - Size of the intermediate layers are specified as list of `hidden_dims`.\n",
    "hidden_dims = [64, 64]\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Set up a simple Double DQN agent\n",
    "# Set up a different instance of a Q value network.\n",
    "\n",
    "# We will be using a one hot representation for representing actions. So take action_dim = num_actions.\n",
    "\n",
    "Q_value_network = VanillaQValueNetwork(state_dim=env.observation_space.shape[0],  # dimension of the state representation\n",
    "                                       action_dim=num_actions,                    # dimension of the action representation\n",
    "                                       hidden_dims=hidden_dims,                   # dimensions of the intermediate layers\n",
    "                                       output_dim=1)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using the 'network_type' argument, use the 'network_instance' argument.\n",
    "# Pass Q_value_network as the `network_instance` to the `DeepQLearning` policy learner.\n",
    "DQNagent = PearlAgent(\n",
    "    policy_learner=DeepQLearning(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        action_space=env.action_space,\n",
    "        batch_size=64,\n",
    "        training_rounds=10,\n",
    "        soft_update_tau=0.001,\n",
    "\n",
    "        network_instance=Q_value_network, # pass an instance of Q value network to the policy learner.\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=num_actions\n",
    "        ),\n",
    "    ),\n",
    "    replay_buffer=FIFOOffPolicyReplayBuffer(10_000),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = online_learning(\n",
    "    agent=DQNagent,\n",
    "    env=env,\n",
    "    number_of_episodes=200,\n",
    "    print_every_x_episodes=100,   # print returns after every 10 episdoes\n",
    "    learn_after_episode=False,    # instead of updating after every environment interaction, Q networks are updates at the end of each episode\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "torch.save(info[\"return\"], \"DQN-return.pt\")   # info[\"return\"] refers to the episodic returns\n",
    "plt.plot(np.arange(len(info[\"return\"])), info[\"return\"], label=\"DQN\")\n",
    "plt.title(\"Episodic returns\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env=TournamentEnv(season=2024,\n",
    "                  exclude_seasons=[c for c in range(2003,2023)],\n",
    "                  team_stats=team_data,\n",
    "                  shuffle=False,\n",
    "                  discrete=True,\n",
    "                  reward_on_round_end=False,\n",
    "                  loading_bar=False\n",
    "                  )\n",
    "test_env=GymEnvironment(test_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=DQNagent\n",
    "observation, action_space = env.reset(season=2024)\n",
    "agent.reset(observation, action_space)\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.act(exploit=False)\n",
    "    action_result = env.step(action)\n",
    "    agent.observe(action_result)\n",
    "    agent.learn()\n",
    "    done = action_result.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a different instance of a Q value network.\n",
    "\n",
    "# We will be using a one hot representation for representing actions. So take action_dim = num_actions.\n",
    "Q_network_DoubleDQN = VanillaQValueNetwork(state_dim=env.observation_space.shape[0],  # dimension of the state representation\n",
    "                                       action_dim=num_actions,                        # dimension of the action representation\n",
    "                                       hidden_dims=hidden_dims,                       # dimensions of the intermediate layers\n",
    "                                       output_dim=1)  \n",
    "# Instead of using the 'network_type' argument, use the 'network_instance' argument.\n",
    "# Pass Q_value_network as the `network_instance` to the `DoubleDQN` policy learner.\n",
    "DoubleDQNagent = PearlAgent(\n",
    "    policy_learner=DoubleDQN(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        action_space=env.action_space,\n",
    "        batch_size=64,\n",
    "        training_rounds=10,\n",
    "        soft_update_tau=0.75,\n",
    "        network_instance=Q_network_DoubleDQN,   # pass an instance of Q value network to the policy learner.\n",
    "        action_representation_module=OneHotActionTensorRepresentationModule(\n",
    "            max_number_actions=num_actions\n",
    "        ),\n",
    "    ),\n",
    "    replay_buffer=FIFOOffPolicyReplayBuffer(10_000),\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The online learning function in Pearl implements environment interaction and learning\n",
    "# and returns a dictionary with episodic returns\n",
    "\n",
    "info_DoubleDQN = online_learning(\n",
    "    agent=DoubleDQNagent,\n",
    "    env=env,\n",
    "    number_of_episodes=2000,\n",
    "    print_every_x_episodes=100,   #  print returns after every 10 episdoes\n",
    "    learn_after_episode=True,    # instead of updating after every environment interaction, Q networks are updates at the end of each episode\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "torch.save(info_DoubleDQN[\"return\"], \"DoubleDQN-return.pt\")   # info[\"return\"] refers to the episodic returns\n",
    "plt.plot(np.arange(len(info_DoubleDQN[\"return\"])), info_DoubleDQN[\"return\"], label=\"DoubleDQN\")\n",
    "plt.title(\"Episodic returns\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=DoubleDQNagent\n",
    "observation, action_space = env.reset()\n",
    "agent.reset(observation, action_space)\n",
    "done = False\n",
    "while not done:\n",
    "    \n",
    "    action = agent.act(exploit=True)\n",
    "    action_result = env.step(action)\n",
    "    agent.observe(action_result)\n",
    "    agent.learn()\n",
    "    done = action_result.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
