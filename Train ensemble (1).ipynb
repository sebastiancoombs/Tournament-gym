{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\standard\\OneDrive\\Documents\\Git\\Tournament-gym\\tournamentgym.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tournamentgym import TournamentEnv\n",
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "import traceback\n",
    "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
    "import hyperopt \n",
    "from pprint import pprint\n",
    "import graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3 import PPO,A2C,DQN\n",
    "from sb3_contrib import RecurrentPPO,ARS,QRDQN\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "## tensor board_log\n",
    "tensorboard_log = \"Logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "team_data=pd.read_csv('Process_data/M_pm_w_names.csv')\n",
    "env=TournamentEnv(season=2022,team_stats=team_data,loading_bar=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Models ->2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': sb3_contrib.ppo_recurrent.ppo_recurrent.RecurrentPPO,\n",
       " 'model_kwargs': {'batch_size': 68,\n",
       "  'gae_lambda': 0.7014687552624854,\n",
       "  'gamma': 0.20752031241878421,\n",
       "  'learning_rate': 0.07635657807640692,\n",
       "  'n_epochs': 18,\n",
       "  'n_steps': 26,\n",
       "  'policy': 'MlpLstmPolicy',\n",
       "  'policy_kwargs': {'net_arch': (256,)}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##hyper param Search\n",
    "space=hp.choice('model_type',\n",
    "    [\n",
    "\n",
    "        {'model':RecurrentPPO,\n",
    "         'model_kwargs':{\n",
    "                    'policy':'MlpLstmPolicy',\n",
    "                    'learning_rate':hp.uniform('PPO_learning_rate',1e-7,0.1),\n",
    "                    'n_steps':hp.choice('PPO_n_steps',[c for c in range(10,200,2)]),\n",
    "                    'gamma':hp.uniform('PPO_gamma',1e-4,1),\n",
    "                    'gae_lambda':hp.uniform('PPO_gae_lambda',1e-4,1),\n",
    "                    'batch_size':hp.choice('PPO_batch_size',[c for c in range(2,200,2)]),\n",
    "                    'n_epochs':hp.choice('PPO_n_epochs',[c for c in range(10,20,2)]),\n",
    "                    'policy_kwargs':{\n",
    "                                    'net_arch':list([hp.choice('PPO_net_arch',[64,128,256])])\n",
    "                                    },\n",
    "\n",
    "                                    # 'lr_schedule':hp.lognormal('PPO_lr_schedule',1e-6,.01),\n",
    "                                    },\n",
    "                },\n",
    "        \n",
    "        # {'model':QRDQN,\n",
    "        #  'model_kwargs':{\n",
    "        #             'policy':'MlpPolicy',\n",
    "\n",
    "        #             'learning_rate':hp.uniform('DQN_learning_rate',1e-7,0.1),\n",
    "        #             'train_freq':hp.choice('DQN_train_freq',[c for c in range(2,68,2)]),\n",
    "\n",
    "        #             'buffer_size':hp.choice('DQN_buffer_size',[c for c in range(2,68,2)]),\n",
    "        #             'batch_size':hp.choice('DQN_batch_size',[c for c in range(2,200,2)]),\n",
    "        #             'tau':hp.uniform('DQN_tau',1e-4,1),\n",
    "        #             'gamma':hp.uniform('DQN_gamma',1e-4,1),\n",
    "\n",
    "        #             'exploration_fraction':hp.uniform('DQN_exploration_fraction',1e-4,1),\n",
    "        #             'exploration_initial_eps':hp.uniform('DQN_exploration_initial_eps',1e-4,1),\n",
    "        #             'exploration_final_eps':hp.uniform('DQN_exploration_final_eps',1e-7,1e-4),\n",
    "\n",
    "        #             'target_update_interval':hp.choice('DQN_target_update_interval',[c for c  in range (2,100,2)]),\n",
    "        #             'policy_kwargs':{\n",
    "        #                             'net_arch':list([hp.choice('DQN_net_arch',[64,128,256])]),\n",
    "        #                             # 'lr_schedule':hp.lognormal('DQN_lr_schedule',1e-6,.01),\n",
    "        #                             },\n",
    "        #         },\n",
    "        #     },\n",
    "\n",
    "        {'model':ARS,\n",
    "         'model_kwargs':{\n",
    "                    'policy':hp.choice('ARS_policy',['MlpPolicy','LinearPolicy']),\n",
    "                    'learning_rate':hp.uniform('ARS_learning_rate',1e-7,0.1),\n",
    "                    'n_delta':hp.choice('ARS_n_delta',[c for c in range(2,68,2)]),\n",
    "\n",
    "                    # 'zero_policy':hp.choice('ARS_zero_policy',[True,False]),\n",
    "                    'policy_kwargs':{\n",
    "                                    'net_arch':list([hp.choice('ARS_net_arch',[64,128,256])])\n",
    "                                    },\n",
    "                                    # 'lr_schedule':hp.lognormal('ARS_lr_schedule',1e-6,.01),\n",
    "                        },\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "sample_space=hyperopt.pyll.stochastic.sample(space)\n",
    "sample_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_options=dict(season=2022,team_stats=team_data,discrete=False,loading_bar=False)\n",
    "test_options=dict(season=2023,team_stats=team_data,discrete=False,loading_bar=False,exclude_seasons=[c for c in range(2003,2023)])\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    try:\n",
    "        num_envs=10\n",
    "\n",
    "        test_multi_env=sb3.common.env_util.make_vec_env(TournamentEnv,n_envs=num_envs,env_kwargs=test_options)\n",
    "        train_multi_env=sb3.common.env_util.make_vec_env(TournamentEnv,n_envs=num_envs,env_kwargs=train_options)\n",
    "\n",
    "        model=space['model']\n",
    "        model_args=space['model_kwargs']\n",
    "        policy=model_args['policy']\n",
    "        if policy=='LinearPolicy':\n",
    "            model_args.pop('policy_kwargs')\n",
    "            \n",
    "        model = model(env=train_multi_env,tensorboard_log='Logs/',device='cuda', verbose=0,**model_args)\n",
    "\n",
    "        # stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=100, min_evals=10, verbose=0)\n",
    "        # eval_callback = EvalCallback(model.env, eval_freq=2,  verbose=0)\n",
    "\n",
    "        model.learn(50000)\n",
    "\n",
    "        model.set_env(test_multi_env)\n",
    "        \n",
    "        obs=model.env.reset()\n",
    "        states=None\n",
    "        episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "        done=[False]\n",
    "        while not done[0]:\n",
    "            acts,states=model.predict(observation=obs,state=states,episode_start=episode_starts,deterministic=True)\n",
    "            obs, reward, done, info = model.env.step(acts)\n",
    "            episode_starts = done\n",
    "\n",
    "        rew=info[0]['score']\n",
    "        return {'loss':float(-rew),'status':STATUS_OK}\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        pprint(space)\n",
    "        traceback.print_exc(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 9/200 [55:13<28:10:07, 530.93s/trial, best loss: -47.0]"
     ]
    }
   ],
   "source": [
    "\n",
    "trails=Trials()\n",
    "best=fmin(fn=objective,\n",
    "         space=space,\n",
    "         algo=tpe.suggest,\n",
    "         max_evals=200,\n",
    "         trials=trails)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(trails, open(\"Day2_trials.pkl\", \"wb\"))\n",
    "trials = pickle.load(open(\"my_trials.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_options=dict(season=2022,team_stats=team_data,loading_bar=False)\n",
    "# best=hyperopt.pyll.stochastic.sample(space)\n",
    "pprint(best)\n",
    "best_model_type=best['model']\n",
    "model_args=best['model_kwargs']\n",
    "policy=model_args['policy']\n",
    "\n",
    "if policy=='LinearPolicy':\n",
    "\n",
    "    model_args.pop('policy_kwargs')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_env=sb3.common.env_util.make_vec_env(TournamentEnv,n_envs=1,env_kwargs=train_options)\n",
    "test_env=sb3.common.env_util.make_vec_env(TournamentEnv,n_envs=1,env_kwargs=test_options)\n",
    "best_model = best_model_type( env=train_env,\n",
    "                                tensorboard_log='Logs/',\n",
    "                                device='cuda',\n",
    "                                **model_args)\n",
    "# stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=100, min_evals=10, verbose=0)\n",
    "eval_callback = EvalCallback(best_model.env, eval_freq=50,  verbose=0)\n",
    "\n",
    "best_model.learn(50000,callback=eval_callback)\n",
    "# best_model.learn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.set_env(test_env)\n",
    "obs=best_model.env.reset()\n",
    "states=None\n",
    "episode_starts = np.ones((1,), dtype=bool)\n",
    "done=[False]\n",
    "while not done[0]:\n",
    "\n",
    "    acts,states=best_model.predict(observation=obs,state=states,episode_start=episode_starts,deterministic=True)\n",
    "    obs, reward, done, info = best_model.env.step(acts)\n",
    "    episode_starts=done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dqn={'batch_size': 37,\n",
    " 'buffer_size': 2,\n",
    " 'exploration_final_eps': 3.450102881484894e-05,\n",
    " 'exploration_fraction': 0.9148110500687865,\n",
    " 'exploration_initial_eps': 0.09837129661293152,\n",
    " 'gamma': 0.2731720156175311,\n",
    " 'learning_rate': 0.025123870821062953,\n",
    " 'net_arch': 0,\n",
    " 'target_update_interval': 15,\n",
    " 'tau': 0.8416411197389558,\n",
    " 'train_freq': 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.set_env(multi_env)\n",
    "\n",
    "env=test_model.env\n",
    "obs=env.reset()\n",
    "done=[False]\n",
    "states=None\n",
    "while not any(done):\n",
    "    acts,states=test_model.predict(obs,state=states,episode_start=done,deterministic=True)\n",
    "\n",
    "    obs,rew,done,info=env.step(acts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[1]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test one 2022\n",
    "obs = test_env.reset(season=2023,weight_scores=False,track_wins=True)\n",
    "done = False\n",
    "while not done:\n",
    "    action = model.predict(obs,deterministic =True)[0]\n",
    "    obs, reward, done, info = test_env.step(action)\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test individually\n",
    "scores=[]\n",
    "max_scores=[]\n",
    "model_scores={}\n",
    "dfs={}\n",
    "for i,model in enumerate(models):\n",
    "\n",
    "    obs = env.reset(season=2022,weight_scores=False,track_wins=False)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = model.predict(obs,deterministic=False)[0]\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "    total_score=info['total_score']\n",
    "    num_correct=info['num_correct']\n",
    "    scores.append(total_score) \n",
    "    dfs[total_score]=env.action_data\n",
    "    model_scores[total_score]=model\n",
    "    name=model.__repr__().split(' ')[0].split('.')[-1]\n",
    "    env.render(score=f'{name} {i} Correct:{num_correct} Score:{total_score}')\n",
    "display(info['total_score'],info['num_correct'])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick out the best Models\n",
    "scores=sorted(scores)\n",
    "topscore=scores[-1]\n",
    "secondscore=scores[-3]\n",
    "print(scores)\n",
    "top_model=model_scores[topscore]\n",
    "second_model=model_scores[secondscore]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict This year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs = env.reset(season=2023,weight_scores=False,track_wins=True)\n",
    "done = False\n",
    "while not done:\n",
    "    actions = [model.predict(obs,deterministic =False)[0] for model in models]\n",
    "    action=np.mean(actions, axis=0)\n",
    "\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "env.render(score=f\"Ensembled ARS Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs = env.reset(season=2023,weight_scores=False,track_wins=False)\n",
    "done = False\n",
    "while not done:\n",
    "    action = top_model.predict(obs,deterministic=False)[0]\n",
    "    obs, reward, done, info = env.step(action)\n",
    "name=top_model.__repr__().split(' ')[0].split('.')[-1]\n",
    "env.render(score=f'Top {name}  Pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(season=2023,weight_scores=False,track_wins=False)\n",
    "done = False\n",
    "while not done:\n",
    "    action = second_model.predict(obs,deterministic=False)[0]\n",
    "    obs, reward, done, info = env.step(action)\n",
    "name=second_model.__repr__().split(' ')[0].split('.')[-1]\n",
    "env.render(score=f'Second Best {name} Pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,model in enumerate(models):\n",
    "\n",
    "#     obs = env.reset(season=2023,weight_scores=False,track_wins=False)\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         action = model.predict(obs,deterministic=True)[0]\n",
    "\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#     env.render(score=f'Model {i} Pred')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jgfulfgu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c02ce77fc9854b14bbf737e9ff58b840b53c17f549f3baecfcf54a3b4065de24"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
